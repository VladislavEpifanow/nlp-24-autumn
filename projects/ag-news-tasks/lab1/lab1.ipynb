{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet(\"../data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet(\"../data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    # (?<!\\w\\.\\w.) - проверяет, что нет слова, за которым следует точка и еще одно слово.\n",
    "    # (?<![A-Z][a-z]\\.) - проверяет, что перед текущей позицией нет заглавной буквы, за которой следует строчная буква и точка.\n",
    "    # (?<=\\.|\\?|!) - проверяет, что перед текущей позицией находится точка, вопросительный знак или восклицательный знак.\n",
    "    # \\s - пробельный символ \n",
    "    sentence_pattern = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s')\n",
    "    \n",
    "    # Находим все предложения в тексте\n",
    "    sentences = sentence_pattern.split(text)\n",
    "    \n",
    "    # Удаляем пустые строки, если они есть\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who are you?', \"How it's going?\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences(\"Who are you? How it's going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emails(sentence):\n",
    "    # [A-Za-z0-9._%+-]+ - часть почты до @\n",
    "    # [A-Za-z0-9.-]+ - доменная часть - .com(или другого)\n",
    "    # [A-Z|a-z]{2,} - доменный уровень (например, .com, .org)\n",
    "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "    \n",
    "    # Находим все email-адреса в тексте\n",
    "    emails = email_pattern.findall(sentence)\n",
    "    \n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user.name@example.com']\n",
      "['another-email@domain.org']\n",
      "['test123@sub.domain.co.uk']\n"
     ]
    }
   ],
   "source": [
    "text = 'Please, send your feedback at this adress: {mail}'\n",
    "emails = [\"user.name@example.com\",\"another-email@domain.org\", \"test123@sub.domain.co.uk\"]\n",
    "for em in emails:\n",
    "    print(find_emails(text.format(mail=em)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phone_number(sentence):\n",
    "    # \\+? - символ + есть ноль или 1 раз\n",
    "    # [- (]? и [- )]? - разделитель в  виде -, () и пробел\n",
    "    # \\d{3} - три любых цифры\n",
    "    # \n",
    "    number_pattern = re.compile(r'\\+?7?[- (]?\\d{3}[- )]?\\d{3}[- ]?\\d{2}[- ]?\\d{2}')\n",
    "    \n",
    "    # Находим все номера телефонов в тексте\n",
    "    phone_numbers = number_pattern.findall(sentence)\n",
    "    \n",
    "    return phone_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+79261234567']\n",
      "[' 8926123456']\n",
      "[' 7926123456']\n",
      "['+7 926 123 45 67']\n",
      "['(926)123-45-67']\n"
     ]
    }
   ],
   "source": [
    "text_phone = 'Please, call us back: {number}'\n",
    "phones = [\"+79261234567\",\"89261234567\", \"79261234567\",\"+7 926 123 45 67\", \"8(926)123-45-67\"]\n",
    "for phone in phones:\n",
    "    print(find_phone_number(text_phone.format(number=phone)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+79261234567']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_phones = \"\"\"Please, call us, to get more information:\n",
    "Additional phone number: +79261234567\n",
    "Email: mail@yandex.com\"\"\"\n",
    "find_phone_number(text_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dates(sentence):\n",
    "    # Регулярное выражение для поиска дат\n",
    "    date_pattern = re.compile(r'\\b(\\d{1,2})([./-]?)(\\d{1,2})\\2(\\d{2,4})\\b')\n",
    "    \n",
    "    # Находим все даты в тексте\n",
    "    dates = date_pattern.findall(sentence)\n",
    "\n",
    "    # Преобразуем найденные даты в строки\n",
    "    formatted_dates = [f\"{day}{separator}{month}{separator}{year}\" for day, separator, month, year in dates]\n",
    "    \n",
    "    return formatted_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19.06.2024']\n",
      "['19-06-2024']\n",
      "['10/12/24']\n"
     ]
    }
   ],
   "source": [
    "text_date = 'Date to meet: {dat}'\n",
    "dates = ['19.06.2024', '19-06-2024', '10/12/24']\n",
    "for date in dates:\n",
    "    print(find_dates(text_date.format(dat=date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    # Регулярное выражение для выделения отдельных токенов\n",
    "    pattern = r\"\\+?\\b[\\w@.]+(?:'\\w+)?\\b|[:;,?.!]\"\n",
    "    return re.findall(pattern, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_sentence(text_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, call us, to get more information:\n",
      "Additional phone number: +79261234567\n",
      "Email: mail@yandex.com\n"
     ]
    }
   ],
   "source": [
    "print(text_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', ',', 'call', 'us', ',', 'to', 'get', 'more', 'information', ':', 'Additional', 'phone', 'number', ':', '+79261234567', 'Email', ':', 'mail@yandex.com']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refridger'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('refridgerator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generat'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('generations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optim'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refridgerator'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('refridgerator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generation'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    sentences = split_into_sentences(text)\n",
    "    annotated_text = ''\n",
    "    for sentence in sentences:\n",
    "        annotated_sentences = ''\n",
    "        word_tokens = tokenize_sentence(sentence)\n",
    "\n",
    "        for token in word_tokens:\n",
    "            emails = find_emails(token)\n",
    "            phones = find_phone_number(token)\n",
    "            dates = find_dates(token)\n",
    "            if emails:\n",
    "                annotated_sentences += email[0] + '\\n'\n",
    "            elif phones:\n",
    "                annotated_sentences += phone[0] + '\\n'\n",
    "            elif dates:\n",
    "                annotated_sentences += dates[0] + '\\n'\n",
    "            else:\n",
    "                stem = stemmer.stem(token)\n",
    "                lemm = lemmatizer.lemmatize(token)\n",
    "                annotated_sentences += '\\t'.join([token, stem, lemm]) + '\\n'\n",
    "        \n",
    "        annotated_text += annotated_sentences + '\\n'\n",
    "\n",
    "    return annotated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n"
     ]
    }
   ],
   "source": [
    "print(train_data['text'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks\tstock\tStocks\n",
      "End\tend\tEnd\n",
      "Up\tup\tUp\n",
      ",\t,\t,\n",
      "But\tbut\tBut\n",
      "Near\tnear\tNear\n",
      "Year\tyear\tYear\n",
      "Lows\tlow\tLows\n",
      "Reuters\treuter\tReuters\n",
      "Reuters\treuter\tReuters\n",
      "Stocks\tstock\tStocks\n",
      "ended\tend\tended\n",
      "slightly\tslight\tslightly\n",
      "higher\thigher\thigher\n",
      "on\ton\ton\n",
      "Friday\tfriday\tFriday\n",
      "but\tbut\tbut\n",
      "stayed\tstay\tstayed\n",
      "near\tnear\tnear\n",
      "lows\tlow\tlow\n",
      "for\tfor\tfor\n",
      "the\tthe\tthe\n",
      "year\tyear\tyear\n",
      "as\tas\ta\n",
      "oil\toil\toil\n",
      "prices\tprice\tprice\n",
      "surged\tsurg\tsurged\n",
      "past\tpast\tpast\n",
      "36\t36\t36\n",
      ";\t;\t;\n",
      "46\t46\t46\n",
      "a\ta\ta\n",
      "barrel\tbarrel\tbarrel\n",
      ",\t,\t,\n",
      "offsetting\toffset\toffsetting\n",
      "a\ta\ta\n",
      "positive\tposit\tpositive\n",
      "outlook\toutlook\toutlook\n",
      "from\tfrom\tfrom\n",
      "computer\tcomput\tcomputer\n",
      "maker\tmaker\tmaker\n",
      "Dell\tdell\tDell\n",
      "Inc\tinc\tInc\n",
      ".\t.\t.\n",
      "\n",
      "DELL.O\tdell.o\tDELL.O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(train_data['text'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(train_data, test_data):\n",
    "    path_to_save = '../assets/annotated-corpus/'\n",
    "    classes = sorted(train_data['label'].unique())\n",
    "    for type, data in [('train', train_data), ('test',test_data)]:\n",
    "        for cl in classes:\n",
    "            folder_name = path_to_save + type + '/' + str(cl)\n",
    "            annotated_file = ''\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "                \n",
    "            for text_item in data.loc[data['label'] == cl]['text'].to_list():\n",
    "                annotated_file += tokenizer(text_item)\n",
    "                \n",
    "            file_path = folder_name + '/' + str(cl) + '.tsv'\n",
    "            with open(file_path, 'w', encoding='utf-8') as tsv_file:\n",
    "                tsv_file.write(annotated_file)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_corpus(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
