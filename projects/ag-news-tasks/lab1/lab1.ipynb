{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(language='english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_parquet(\"../data/train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_parquet(\"../data/test.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 2)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_sentences(text):\n",
    "    # (?<!\\w\\.\\w.) - проверяет, что нет слова, за которым следует точка и еще одно слово.\n",
    "    # (?<![A-Z][a-z]\\.) - проверяет, что перед текущей позицией нет заглавной буквы, за которой следует строчная буква и точка.\n",
    "    # (?<=\\.|\\?|!) - проверяет, что перед текущей позицией находится точка, вопросительный знак или восклицательный знак.\n",
    "    # \\s - пробельный символ \n",
    "    sentence_pattern = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s')\n",
    "    \n",
    "    # Находим все предложения в тексте\n",
    "    sentences = sentence_pattern.split(text)\n",
    "    \n",
    "    # Удаляем пустые строки, если они есть\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Who are you?', \"How it's going?\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_into_sentences(\"Who are you? How it's going?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_emails(sentence):\n",
    "    # [A-Za-z0-9._%+-]+ - часть почты до @\n",
    "    # [A-Za-z0-9.-]+ - доменная часть - .com(или другого)\n",
    "    # [A-Z|a-z]{2,} - доменный уровень (например, .com, .org)\n",
    "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "    \n",
    "    # Находим все email-адреса в тексте\n",
    "    emails = email_pattern.findall(sentence)\n",
    "    \n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['user.name@example.com']\n",
      "['another-email@domain.org']\n",
      "['test123@sub.domain.co.uk']\n"
     ]
    }
   ],
   "source": [
    "text = 'Please, send your feedback at this adress: {mail}'\n",
    "emails = [\"user.name@example.com\",\"another-email@domain.org\", \"test123@sub.domain.co.uk\"]\n",
    "for em in emails:\n",
    "    print(find_emails(text.format(mail=em)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_phone_number(sentence):\n",
    "    # \\+? - символ + есть ноль или 1 раз\n",
    "    # [- (]? и [- )]? - разделитель в  виде -, () и пробел\n",
    "    # \\d{3} - три любых цифры\n",
    "    # \n",
    "    number_pattern = re.compile(r'\\+?7?[- (]?\\d{3}[- )]?\\d{3}[- ]?\\d{2}[- ]?\\d{2}')\n",
    "    \n",
    "    # Находим все номера телефонов в тексте\n",
    "    phone_numbers = number_pattern.findall(sentence)\n",
    "    \n",
    "    return phone_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['+79261234567']\n",
      "[' 8926123456']\n",
      "[' 7926123456']\n",
      "['+7 926 123 45 67']\n",
      "['(926)123-45-67']\n"
     ]
    }
   ],
   "source": [
    "text_phone = 'Please, call us back: {number}'\n",
    "phones = [\"+79261234567\",\"89261234567\", \"79261234567\",\"+7 926 123 45 67\", \"8(926)123-45-67\"]\n",
    "for phone in phones:\n",
    "    print(find_phone_number(text_phone.format(number=phone)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['+79261234567']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_phones = \"\"\"Please, call us, to get more information:\n",
    "Additional phone number: +79261234567\n",
    "Email: mail@yandex.com\"\"\"\n",
    "find_phone_number(text_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dates(sentence):\n",
    "    # Регулярное выражение для поиска дат\n",
    "    date_pattern = re.compile(r'\\b(\\d{1,2})([./-]?)(\\d{1,2})\\2(\\d{2,4})\\b')\n",
    "    \n",
    "    # Находим все даты в тексте\n",
    "    dates = date_pattern.findall(sentence)\n",
    "\n",
    "    # Преобразуем найденные даты в строки\n",
    "    formatted_dates = [f\"{day}{separator}{month}{separator}{year}\" for day, separator, month, year in dates]\n",
    "    \n",
    "    return formatted_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['19.06.2024']\n",
      "['19-06-2024']\n",
      "['10/12/24']\n"
     ]
    }
   ],
   "source": [
    "text_date = 'Date to meet: {dat}'\n",
    "dates = ['19.06.2024', '19-06-2024', '10/12/24']\n",
    "for date in dates:\n",
    "    print(find_dates(text_date.format(dat=date)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    # Регулярное выражение для выделения отдельных токенов\n",
    "    pattern = r\"\\+?\\b[\\w@.]+(?:'\\w+)?\\b|[:;,?.!]\"\n",
    "    return re.findall(pattern, sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize_sentence(text_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please, call us, to get more information:\n",
      "Additional phone number: +79261234567\n",
      "Email: mail@yandex.com\n"
     ]
    }
   ],
   "source": [
    "print(text_phones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Please', ',', 'call', 'us', ',', 'to', 'get', 'more', 'information', ':', 'Additional', 'phone', 'number', ':', '+79261234567', 'Email', ':', 'mail@yandex.com']\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refridger'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('refridgerator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generat'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('generations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'optim'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('optimization')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'refridgerator'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('refridgerator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'generation'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('generation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    sentences = split_into_sentences(text)\n",
    "    tokenized = []  # Список для хранения результатов по каждому предложению\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_data = {\n",
    "            'tokens': [],  # Оригинальные токены\n",
    "            'stems': [],   # Стемы\n",
    "            'lemmas': [],  # Леммы\n",
    "            'entities': []  # Специальные сущности (email, phone, date)\n",
    "        }\n",
    "        \n",
    "        word_tokens = tokenize_sentence(sentence)\n",
    "        \n",
    "        for token in word_tokens:\n",
    "            # Поиск специальных сущностей\n",
    "            emails = find_emails(token)\n",
    "            if emails:\n",
    "                sentence_data['entities'].append({'type': 'email', 'value': emails[0]})\n",
    "                continue\n",
    "            \n",
    "            phones = find_phone_number(token)\n",
    "            if phones:\n",
    "                sentence_data['entities'].append({'type': 'phone', 'value': phones[0]})\n",
    "                continue\n",
    "            \n",
    "            dates = find_dates(token)\n",
    "            if dates:\n",
    "                sentence_data['entities'].append({'type': 'date', 'value': dates[0]})\n",
    "                continue\n",
    "            \n",
    "            # Обычная обработка токена\n",
    "            stem = stemmer.stem(token)\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            \n",
    "            sentence_data['tokens'].append(token)\n",
    "            sentence_data['stems'].append(stem)\n",
    "            sentence_data['lemmas'].append(lemma)\n",
    "        \n",
    "        tokenized.append(sentence_data)  # Добавляем результат предложения в общий список\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_to_tsv(tokenized_data):\n",
    "    tsv_lines = []\n",
    "    \n",
    "    for sentence_data in tokenized_data:\n",
    "        # Обрабатываем обычные токены\n",
    "        for token, stem, lemma in zip(sentence_data['tokens'], sentence_data['stems'], sentence_data['lemmas']):\n",
    "            tsv_lines.append(f\"{token}\\t{stem}\\t{lemma}\")\n",
    "        \n",
    "        # Обрабатываем специальные сущности\n",
    "        for entity in sentence_data['entities']:\n",
    "            tsv_lines.append(f\"{entity['type']}\\t{entity['value']}\")\n",
    "        \n",
    "        # Добавляем разделитель предложений\n",
    "        tsv_lines.append('')\n",
    "    \n",
    "    # Убираем последний лишний разделитель\n",
    "    return '\\n'.join(tsv_lines[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hip Hop's Online Shop Celebrity fashion is booming. These webpreneurs are bringing it to main street\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['text'][55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenize_text(train_data['text'][55])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_tokens = [token for sentence_data in tokenized_data for token in sentence_data['tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hip\thip\tHip\n",
      "Hop's\thop\tHop's\n",
      "Online\tonlin\tOnline\n",
      "Shop\tshop\tShop\n",
      "Celebrity\tcelebr\tCelebrity\n",
      "fashion\tfashion\tfashion\n",
      "is\tis\tis\n",
      "booming\tboom\tbooming\n",
      ".\t.\t.\n",
      "\n",
      "These\tthese\tThese\n",
      "webpreneurs\twebpreneur\twebpreneurs\n",
      "are\tare\tare\n",
      "bringing\tbring\tbringing\n",
      "it\tit\tit\n",
      "to\tto\tto\n",
      "main\tmain\tmain\n",
      "street\tstreet\tstreet\n"
     ]
    }
   ],
   "source": [
    "tsv_output = format_to_tsv(tokenized_data)\n",
    "print(tsv_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n"
     ]
    }
   ],
   "source": [
    "print(train_data['text'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stocks\tstock\tStocks\n",
      "End\tend\tEnd\n",
      "Up\tup\tUp\n",
      ",\t,\t,\n",
      "But\tbut\tBut\n",
      "Near\tnear\tNear\n",
      "Year\tyear\tYear\n",
      "Lows\tlow\tLows\n",
      "Reuters\treuter\tReuters\n",
      "Reuters\treuter\tReuters\n",
      "Stocks\tstock\tStocks\n",
      "ended\tend\tended\n",
      "slightly\tslight\tslightly\n",
      "higher\thigher\thigher\n",
      "on\ton\ton\n",
      "Friday\tfriday\tFriday\n",
      "but\tbut\tbut\n",
      "stayed\tstay\tstayed\n",
      "near\tnear\tnear\n",
      "lows\tlow\tlow\n",
      "for\tfor\tfor\n",
      "the\tthe\tthe\n",
      "year\tyear\tyear\n",
      "as\tas\ta\n",
      "oil\toil\toil\n",
      "prices\tprice\tprice\n",
      "surged\tsurg\tsurged\n",
      "past\tpast\tpast\n",
      "36\t36\t36\n",
      ";\t;\t;\n",
      "46\t46\t46\n",
      "a\ta\ta\n",
      "barrel\tbarrel\tbarrel\n",
      ",\t,\t,\n",
      "offsetting\toffset\toffsetting\n",
      "a\ta\ta\n",
      "positive\tposit\tpositive\n",
      "outlook\toutlook\toutlook\n",
      "from\tfrom\tfrom\n",
      "computer\tcomput\tcomputer\n",
      "maker\tmaker\tmaker\n",
      "Dell\tdell\tDell\n",
      "Inc\tinc\tInc\n",
      ".\t.\t.\n",
      "\n",
      "DELL.O\tdell.o\tDELL.O\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(train_data['text'][5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_corpus(train_data, test_data):\n",
    "    path_to_save = '../assets/annotated-corpus/'\n",
    "    classes = sorted(train_data['label'].unique())\n",
    "    for type, data in [('train', train_data), ('test',test_data)]:\n",
    "        for cl in classes:\n",
    "            folder_name = path_to_save + type + '/' + str(cl)\n",
    "            annotated_file = ''\n",
    "            if not os.path.exists(folder_name):\n",
    "                os.makedirs(folder_name)\n",
    "                \n",
    "            for text_item in data.loc[data['label'] == cl]['text'].to_list():\n",
    "                annotated_file += tokenizer(text_item)\n",
    "                \n",
    "            file_path = folder_name + '/' + str(cl) + '.tsv'\n",
    "            with open(file_path, 'w', encoding='utf-8') as tsv_file:\n",
    "                tsv_file.write(annotated_file)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_corpus(train_data, test_data):\n",
    "    path_to_save = '../assets/annotated-corpus/'\n",
    "    # Получаем все уникальные классы из train и test\n",
    "    train_classes = set(train_data['label'].unique())\n",
    "    test_classes = set(test_data['label'].unique())\n",
    "    all_classes = sorted(list(train_classes.union(test_classes)))\n",
    "    \n",
    "    for data_type, data in [('train', train_data), ('test', test_data)]:\n",
    "        # Создаем копию данных, чтобы не изменять оригинальные\n",
    "        data = data.copy()\n",
    "        # Проверяем наличие doc_id, генерируем при необходимости\n",
    "        if 'doc_id' not in data.columns:\n",
    "            data.reset_index(drop=True, inplace=True)\n",
    "            data['doc_id'] = data.index.astype(str)\n",
    "        \n",
    "        # Проходим по всем классам\n",
    "        for cl in all_classes:\n",
    "            # Фильтруем данные по текущему классу\n",
    "            class_data = data[data['label'] == cl]\n",
    "            if class_data.empty:\n",
    "                continue  # Пропускаем, если нет документов\n",
    "            \n",
    "            # Создаем папку для класса\n",
    "            class_folder = os.path.join(path_to_save, data_type, str(cl))\n",
    "            os.makedirs(class_folder, exist_ok=True)\n",
    "            \n",
    "            # Сохраняем каждый документ отдельным файлом\n",
    "            for _, row in class_data.iterrows():\n",
    "                doc_id = row['doc_id']\n",
    "                text_content = tokenizer(row['text'])\n",
    "                file_path = os.path.join(class_folder, f\"{doc_id}.tsv\")\n",
    "                with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                    file.write(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_corpus(train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
