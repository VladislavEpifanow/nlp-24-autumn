{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f10c2899-4c6b-4fae-b798-697874280d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "804d7023-6f9e-40a4-b0ca-98e068123218",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [WinError 10054]\n",
      "[nltk_data]     Удаленный хост принудительно разорвал существующее\n",
      "[nltk_data]     подключение>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77791c2b-c6ca-4104-9f40-96b49191c294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(tokens):\n",
    "    pattern = r'[^\\w\\s]'\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        clean_token = re.sub(pattern, '', str(token).lower())\n",
    "        if clean_token != '' and clean_token not in stop_words:\n",
    "            cleaned_tokens.append(clean_token)\n",
    "        \n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c94d02d7-b684-4687-912a-d5999ff82493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_tsv_sentences(file_path: str) -> list[list[str]]:\n",
    "    df = pd.read_csv(file_path, sep='\\t', usecols=[0], header=None, names=['word'], skip_blank_lines=False, na_filter=False)\n",
    "    words = df['word'].astype(str).str.strip().tolist()\n",
    "    \n",
    "    sentences = []\n",
    "    current_sentence = []\n",
    "    \n",
    "    for word in words:\n",
    "        if not word:  # Пустая строка в первом столбце\n",
    "            if current_sentence:\n",
    "                sentences.append(current_sentence)\n",
    "                current_sentence = []\n",
    "        else:\n",
    "            current_sentence.append(word)\n",
    "    \n",
    "    # Добавляем последнее предложение, если оно есть\n",
    "    if current_sentence:\n",
    "        sentences.append(current_sentence)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b32181-7063-445d-a38d-14f231877c54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Datasets:   0%|                                                                                  | 0/2 [00:00<?, ?it/s]\n",
      "Classes (train):   0%|                                                                           | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Classes (train):  25%|████████████████▌                                                 | 1/4 [02:41<08:05, 161.79s/it]\u001b[A\n",
      "Classes (train):  50%|█████████████████████████████████                                 | 2/4 [05:25<05:25, 162.84s/it]\u001b[A\n",
      "Classes (train):  75%|█████████████████████████████████████████████████▌                | 3/4 [08:20<02:48, 168.24s/it]\u001b[A\n",
      "Classes (train): 100%|██████████████████████████████████████████████████████████████████| 4/4 [11:21<00:00, 170.48s/it]\u001b[A\n",
      "Datasets:  50%|████████████████████████████████████▌                                    | 1/2 [11:21<11:21, 681.96s/it]\n",
      "Classes (test):   0%|                                                                            | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      "Classes (test):  25%|█████████████████                                                   | 1/4 [00:16<00:48, 16.10s/it]\u001b[A\n",
      "Classes (test):  50%|██████████████████████████████████                                  | 2/4 [00:26<00:25, 12.51s/it]\u001b[A\n",
      "Classes (test):  75%|███████████████████████████████████████████████████                 | 3/4 [00:38<00:12, 12.51s/it]\u001b[A\n",
      "Classes (test): 100%|████████████████████████████████████████████████████████████████████| 4/4 [00:50<00:00, 12.54s/it]\u001b[A\n",
      "Datasets: 100%|█████████████████████████████████████████████████████████████████████████| 2/2 [12:12<00:00, 366.08s/it]\n"
     ]
    }
   ],
   "source": [
    "train_data = []\n",
    "test_data = []\n",
    "base_path = '../assets/annotated-corpus/'\n",
    "\n",
    "# Обработка данных\n",
    "for data_type in tqdm(['train', 'test'], desc='Datasets'):\n",
    "    data_path = os.path.join(base_path, data_type)\n",
    "    \n",
    "    if not os.path.exists(data_path):\n",
    "        continue\n",
    "        \n",
    "    classes = ['0', '1', '2', '3']\n",
    "    \n",
    "    for class_name in tqdm(classes, desc=f'Classes ({data_type})'):\n",
    "        class_path = os.path.join(data_path, class_name)\n",
    "        \n",
    "        docs = [\n",
    "            f for f in os.listdir(class_path) \n",
    "            if f.endswith('.tsv') and os.path.isfile(os.path.join(class_path, f))\n",
    "        ]\n",
    "        \n",
    "        for doc_name in docs:\n",
    "            doc_path = os.path.join(class_path, doc_name)\n",
    "            doc_id = os.path.splitext(doc_name)[0]  # Извлекаем doc_id без расширения\n",
    "            sentences = read_tsv_sentences(doc_path)\n",
    "            \n",
    "            entry = {\n",
    "                \"doc_id\": doc_id,\n",
    "                \"class\": class_name,\n",
    "                \"words\": sentences\n",
    "            }\n",
    "            \n",
    "            if data_type == 'train':\n",
    "                train_data.append(entry)\n",
    "            else:\n",
    "                test_data.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66292651-f31e-4167-89de-313bbee5e556",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"word2vec_v2.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fca509c2-6a02-4e96-8bd8-011d7472500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(train_data, model_path='word2vec.model'):    \n",
    "    model = Word2Vec(\n",
    "        vector_size=100,\n",
    "        window=5,\n",
    "        min_count=1,\n",
    "        workers=4,\n",
    "        sg=1,\n",
    "        alpha=0.025,\n",
    "        min_alpha=0.0001\n",
    "    )\n",
    "    \n",
    "    print(\"\\nПостроение словаря...\")\n",
    "    model.build_vocab(train_data)\n",
    "    \n",
    "    print(\"\\nНачало обучения...\")\n",
    "    model.train(\n",
    "        train_data,\n",
    "        total_examples=model.corpus_count,\n",
    "        epochs=10,\n",
    "        compute_loss=True,\n",
    "        report_delay=1\n",
    "    )\n",
    "    \n",
    "    model.save(model_path)\n",
    "    print(f\"\\nМодель сохранена в {model_path}\")\n",
    "    print(f\"Размер словаря: {len(model.wv.key_to_index)}\")\n",
    "    print(f\"Примеры слов: {list(model.wv.key_to_index.keys())[:10]}\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7d4562fe-7bc8-44df-9cd9-30810e57fac4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': '100022',\n",
       " 'class': '0',\n",
       " 'words': [['Iraqi',\n",
       "   'Prime',\n",
       "   'Minister',\n",
       "   'Encouraged',\n",
       "   'to',\n",
       "   'Meet',\n",
       "   'Opponents',\n",
       "   'An',\n",
       "   'international',\n",
       "   'conference',\n",
       "   'on',\n",
       "   'Iraq',\n",
       "   'is',\n",
       "   'expected',\n",
       "   'to',\n",
       "   'call',\n",
       "   'on',\n",
       "   'the',\n",
       "   'government',\n",
       "   'of',\n",
       "   'Prime',\n",
       "   'Minister',\n",
       "   'Ayad',\n",
       "   'Allawi',\n",
       "   'to',\n",
       "   'meet',\n",
       "   'with',\n",
       "   'its',\n",
       "   'political',\n",
       "   'opponents',\n",
       "   'to',\n",
       "   'encourage',\n",
       "   'them',\n",
       "   'to',\n",
       "   'participate',\n",
       "   'in',\n",
       "   'the',\n",
       "   \"country's\",\n",
       "   'first',\n",
       "   'democratic',\n",
       "   'elections',\n",
       "   'in',\n",
       "   'January',\n",
       "   ',',\n",
       "   'according',\n",
       "   'to',\n",
       "   'a',\n",
       "   'draft',\n",
       "   'of',\n",
       "   'the',\n",
       "   \"conference's\",\n",
       "   'final',\n",
       "   'communique',\n",
       "   '.']]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dd183366-db4b-426b-b919-e58a43ba3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_sentences = [inner_list for item in train_data for inner_list in item['words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f08167dd-19e6-462a-8a67-ee9a21990f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170305"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5580b595-bf04-4f09-8fd6-e698ae0423e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'doc_id': '10000',\n",
       " 'class': '0',\n",
       " 'words': [['A',\n",
       "   'Daily',\n",
       "   'Look',\n",
       "   'at',\n",
       "   'U.S',\n",
       "   '.',\n",
       "   'Iraq',\n",
       "   'Military',\n",
       "   'Deaths',\n",
       "   'AP',\n",
       "   'AP',\n",
       "   'As',\n",
       "   'of',\n",
       "   'Wednesday',\n",
       "   ',',\n",
       "   'Aug',\n",
       "   '.'],\n",
       "  ['25',\n",
       "   ',',\n",
       "   '964',\n",
       "   'U.S',\n",
       "   '.',\n",
       "   'service',\n",
       "   'members',\n",
       "   'have',\n",
       "   'died',\n",
       "   'since',\n",
       "   'the',\n",
       "   'beginning',\n",
       "   'of',\n",
       "   'military',\n",
       "   'operations',\n",
       "   'in',\n",
       "   'Iraq',\n",
       "   'in',\n",
       "   'March',\n",
       "   '2003',\n",
       "   ',',\n",
       "   'according',\n",
       "   'to',\n",
       "   'the',\n",
       "   'Defense',\n",
       "   'Department',\n",
       "   '.'],\n",
       "  ['Of',\n",
       "   'those',\n",
       "   ',',\n",
       "   '722',\n",
       "   'died',\n",
       "   'as',\n",
       "   'a',\n",
       "   'result',\n",
       "   'of',\n",
       "   'hostile',\n",
       "   'action',\n",
       "   'and',\n",
       "   '242',\n",
       "   'died',\n",
       "   'of',\n",
       "   'non',\n",
       "   'hostile',\n",
       "   'causes',\n",
       "   '.']]}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]['"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55194a4d-44c7-4eb7-bc8e-f616f3d4e0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Построение словаря...\n",
      "\n",
      "Начало обучения...\n",
      "\n",
      "Модель сохранена в word2vec_v2.model\n",
      "Размер словаря: 94156\n",
      "Примеры слов: ['.', 'the', ',', 'to', 'a', 'of', 'in', ';', 'and', 'on']\n"
     ]
    }
   ],
   "source": [
    "model = train_word2vec(train_data_sentences, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2257d370-5b53-40b6-8149-9e2dcb2009ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, List\n",
    "\n",
    "def get_word_vectors(words, model, zero_vector_for_unknown=True):\n",
    "    if isinstance(words, str):\n",
    "        words = [words]\n",
    "        \n",
    "    vectors = []\n",
    "    vector_size = model.vector_size if hasattr(model, 'vector_size') else 100\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model.wv:\n",
    "            vectors.append(model.wv[word])\n",
    "        else:\n",
    "            if zero_vector_for_unknown:\n",
    "                vectors.append(np.zeros(vector_size))\n",
    "            else:\n",
    "                continue  # Пропускаем отсутствующие слова\n",
    "    \n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce327f46-a67e-494f-a5f7-8d4207ca8d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Реализация косинусного расстояния\n",
    "def cosine_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    \"\"\"Собственная реализация косинусного расстояния\"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm1 = np.linalg.norm(vec1)\n",
    "    norm2 = np.linalg.norm(vec2)\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 1.0  # Максимальное расстояние для нулевых векторов\n",
    "    \n",
    "    return 1 - (dot_product / (norm1 * norm2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fb5a41b-7814-44de-9e86-56fefa846823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Тестовые данные для анализа\n",
    "test_words = {\n",
    "    'press': {\n",
    "        'similar': ['media', 'journalism', 'news'],\n",
    "        'related': ['article', 'reporter', 'headline'],\n",
    "        'different': ['mountain', 'guitar', 'painting']\n",
    "    },\n",
    "    'government': {\n",
    "        'similar': ['administration', 'authority', 'regime'],\n",
    "        'related': ['politics', 'law', 'policy'],\n",
    "        'different': ['flower', 'bicycle', 'music']\n",
    "    },\n",
    "    'crime': {\n",
    "        'similar': ['offense', 'violation', 'felony'],\n",
    "        'related': ['police', 'law', 'punishment'],\n",
    "        'different': ['sunshine', 'happiness', 'rainbow']\n",
    "    },\n",
    "    'troops': {\n",
    "        'similar': ['soldiers', 'military', 'army'],\n",
    "        'related': ['war', 'defense', 'combat'],\n",
    "        'different': ['peace', 'butterfly', 'garden']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "941ca8a3-d14c-4b72-bdf0-4e73c453ce0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Функция для анализа расстояний\n",
    "def analyze_distances(word, model, test_words):\n",
    "    if word not in model.wv:\n",
    "        raise ValueError(f\"Слово '{word}' отсутствует в модели\")\n",
    "    \n",
    "    main_vector = model.wv[word]\n",
    "    distances = {'similar': [], 'related': [], 'different': []}\n",
    "    \n",
    "    for group, words in test_words[word].items():\n",
    "        for w in words:\n",
    "            if w in model.wv:\n",
    "                dist = cosine_distance(main_vector, model.wv[w])\n",
    "                distances[group].append(dist)\n",
    "    \n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "131c2031-2d07-4df8-83f2-31800a15f22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for word in test_words:\n",
    "    distances = analyze_distances(word, model, test_words)\n",
    "    metrics[word] = {\n",
    "        'similar_mean': np.mean(distances['similar']),\n",
    "        'related_mean': np.mean(distances['related']),\n",
    "        'different_mean': np.mean(distances['different']),\n",
    "        'diff_score': np.mean(distances['different']) - np.mean(distances['similar'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed06636d-dc78-4574-89ef-449a7d634111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'press': {'similar_mean': 0.5175909896691641,\n",
       "  'related_mean': 0.595216711362203,\n",
       "  'different_mean': 0.7673713614543279,\n",
       "  'diff_score': 0.24978037178516388},\n",
       " 'government': {'similar_mean': 0.4068453907966614,\n",
       "  'related_mean': 0.4919247627258301,\n",
       "  'different_mean': 0.7369668434063593,\n",
       "  'diff_score': 0.33012145260969794},\n",
       " 'crime': {'similar_mean': 0.577508936325709,\n",
       "  'related_mean': 0.4980180859565735,\n",
       "  'different_mean': 0.6868096192677816,\n",
       "  'diff_score': 0.10930068294207262},\n",
       " 'troops': {'similar_mean': 0.29604480663935345,\n",
       "  'related_mean': 0.5652153690656027,\n",
       "  'different_mean': 0.6560452431440353,\n",
       "  'diff_score': 0.3600004365046819}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9200db3a-ca67-43ee-a2df-e68026bead5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Из лабы 1\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    # (?<!\\w\\.\\w.) - проверяет, что нет слова, за которым следует точка и еще одно слово.\n",
    "    # (?<![A-Z][a-z]\\.) - проверяет, что перед текущей позицией нет заглавной буквы, за которой следует строчная буква и точка.\n",
    "    # (?<=\\.|\\?|!) - проверяет, что перед текущей позицией находится точка, вопросительный знак или восклицательный знак.\n",
    "    # \\s - пробельный символ \n",
    "    sentence_pattern = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!)\\s')\n",
    "    \n",
    "    # Находим все предложения в тексте\n",
    "    sentences = sentence_pattern.split(text)\n",
    "    \n",
    "    # Удаляем пустые строки, если они есть\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def tokenize_sentence(sentence):\n",
    "    # Регулярное выражение для выделения отдельных токенов\n",
    "    pattern = r\"\\+?\\b[\\w@.]+(?:'\\w+)?\\b|[:;,?.!]\"\n",
    "    return re.findall(pattern, sentence)\n",
    "\n",
    "def find_emails(sentence):\n",
    "    # [A-Za-z0-9._%+-]+ - часть почты до @\n",
    "    # [A-Za-z0-9.-]+ - доменная часть - .com(или другого)\n",
    "    # [A-Z|a-z]{2,} - доменный уровень (например, .com, .org)\n",
    "    email_pattern = re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b')\n",
    "    \n",
    "    # Находим все email-адреса в тексте\n",
    "    emails = email_pattern.findall(sentence)\n",
    "    \n",
    "    return emails\n",
    "\n",
    "def find_phone_number(sentence):\n",
    "    # \\+? - символ + есть ноль или 1 раз\n",
    "    # [- (]? и [- )]? - разделитель в  виде -, () и пробел\n",
    "    # \\d{3} - три любых цифры\n",
    "    # \n",
    "    number_pattern = re.compile(r'\\+?7?[- (]?\\d{3}[- )]?\\d{3}[- ]?\\d{2}[- ]?\\d{2}')\n",
    "    \n",
    "    # Находим все номера телефонов в тексте\n",
    "    phone_numbers = number_pattern.findall(sentence)\n",
    "    \n",
    "    return phone_numbers\n",
    "\n",
    "def find_dates(sentence):\n",
    "    # Регулярное выражение для поиска дат\n",
    "    date_pattern = re.compile(r'\\b(\\d{1,2})([./-]?)(\\d{1,2})\\2(\\d{2,4})\\b')\n",
    "    \n",
    "    # Находим все даты в тексте\n",
    "    dates = date_pattern.findall(sentence)\n",
    "\n",
    "    # Преобразуем найденные даты в строки\n",
    "    formatted_dates = [f\"{day}{separator}{month}{separator}{year}\" for day, separator, month, year in dates]\n",
    "    \n",
    "    return formatted_dates\n",
    "\n",
    "def tokenize_text(text):\n",
    "    sentences = split_into_sentences(text)\n",
    "    tokenized = []  # Список для хранения результатов по каждому предложению\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        sentence_data = {\n",
    "            'tokens': [],  # Оригинальные токены\n",
    "            'stems': [],   # Стемы\n",
    "            'lemmas': [],  # Леммы\n",
    "            'entities': []  # Специальные сущности (email, phone, date)\n",
    "        }\n",
    "        \n",
    "        word_tokens = tokenize_sentence(sentence)\n",
    "        \n",
    "        for token in word_tokens:\n",
    "            # Поиск специальных сущностей\n",
    "            emails = find_emails(token)\n",
    "            if emails:\n",
    "                sentence_data['entities'].append({'type': 'email', 'value': emails[0]})\n",
    "                continue\n",
    "            \n",
    "            phones = find_phone_number(token)\n",
    "            if phones:\n",
    "                sentence_data['entities'].append({'type': 'phone', 'value': phones[0]})\n",
    "                continue\n",
    "            \n",
    "            dates = find_dates(token)\n",
    "            if dates:\n",
    "                sentence_data['entities'].append({'type': 'date', 'value': dates[0]})\n",
    "                continue\n",
    "            \n",
    "            # Обычная обработка токена\n",
    "            stem = stemmer.stem(token)\n",
    "            lemma = lemmatizer.lemmatize(token)\n",
    "            \n",
    "            sentence_data['tokens'].append(token)\n",
    "            sentence_data['stems'].append(stem)\n",
    "            sentence_data['lemmas'].append(lemma)\n",
    "        \n",
    "        tokenized.append(sentence_data)  # Добавляем результат предложения в общий список\n",
    "    \n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c51d7ac7-18f3-44f8-87bd-ac5e486d4825",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'This is an example of text. Second sentence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9457fb46-2f8f-4539-99b7-5ffc4d156c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tokens': ['This', 'is', 'an', 'example', 'of', 'text', '.'],\n",
       "  'stems': ['this', 'is', 'an', 'exampl', 'of', 'text', '.'],\n",
       "  'lemmas': ['This', 'is', 'an', 'example', 'of', 'text', '.'],\n",
       "  'entities': []},\n",
       " {'tokens': ['Second', 'sentence', '.'],\n",
       "  'stems': ['second', 'sentenc', '.'],\n",
       "  'lemmas': ['Second', 'sentence', '.'],\n",
       "  'entities': []}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "81ce4951-cc5c-4be4-ae21-a45514f84d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_text(text, model, tokenize=True):\n",
    "    if tokenize:\n",
    "        tokenized_sentences = tokenize_text(text)\n",
    "    else:\n",
    "        tokenized_sentences = text\n",
    "    vector_size = model.vector_size if hasattr(model, 'vector_size') else 100\n",
    "    \n",
    "    doc_token_vectors = []\n",
    "    doc_sentence_vectors = []\n",
    "    doc_token_level_vectors = []\n",
    "    \n",
    "    for sentence_data in tokenized_sentences:\n",
    "        # Векторизация токенов\n",
    "        tokens = sentence_data['tokens'] if tokenize else sentence_data\n",
    "        token_vectors = get_word_vectors(tokens, model)\n",
    "        \n",
    "        # Сохраняем вектора токенов для текущего предложения\n",
    "        doc_token_level_vectors.append(token_vectors)\n",
    "        \n",
    "        # Рассчитываем средний вектор предложения\n",
    "        if len(token_vectors) > 0:\n",
    "            sentence_vector = np.mean(token_vectors, axis=0)\n",
    "        else:\n",
    "            sentence_vector = np.zeros(vector_size)\n",
    "        \n",
    "        doc_sentence_vectors.append(sentence_vector)\n",
    "        doc_token_vectors.extend(token_vectors)  # Все токены документа\n",
    "    \n",
    "    # Рассчитываем средние векторы\n",
    "    result = {\n",
    "        'sentence_vectors': doc_sentence_vectors,\n",
    "        'token_vectors': doc_token_level_vectors\n",
    "    }\n",
    "    \n",
    "    # Вектор всего документа (среднее по предложениям)\n",
    "    if len(doc_sentence_vectors) > 0:\n",
    "        result['document_vector'] = np.mean(doc_sentence_vectors, axis=0)\n",
    "    else:\n",
    "        result['document_vector'] = np.zeros(vector_size)\n",
    "    \n",
    "    # Дополнительно: среднее по всем токенам документа (альтернативный подход)\n",
    "    if len(doc_token_vectors) > 0:\n",
    "        result['document_vector_alt'] = np.mean(doc_token_vectors, axis=0)\n",
    "    else:\n",
    "        result['document_vector_alt'] = np.zeros(vector_size)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3974b00c-a285-4e93-905a-1c385de4f086",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_text = vectorize_text(train_data[0]['words'], model, tokenize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "46bea536-f240-49e9-be7d-a23e3fb3b273",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a22f1506-f75d-4817-8377-c45e0e26dd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_and_save_dataset(model, train_data, test_data, output_dir=\"../assets/annotated-corpus-vectorized/\"):\n",
    "    def _process_subset(data, subset_name):\n",
    "        with tqdm(total=len(data), desc=f\"Processing {subset_name} subset\") as pbar:\n",
    "            for doc in data:\n",
    "                # Векторизация документа\n",
    "                vectorization_result = vectorize_text(doc[\"words\"], model, tokenize=False)\n",
    "                doc_vector = vectorization_result[\"document_vector\"]\n",
    "                \n",
    "                class_dir = os.path.join(output_dir, subset_name, doc[\"class\"])\n",
    "                os.makedirs(class_dir, exist_ok=True)\n",
    "                \n",
    "                # Сохранение в файл\n",
    "                filename = os.path.join(class_dir, f\"{doc['doc_id']}.tsv\")\n",
    "                with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "                    vector_str = \"\\t\".join(map(\"{:.6f}\".format, doc_vector))\n",
    "                    f.write(f\"{doc['doc_id']}\\t{vector_str}\\n\")\n",
    "                \n",
    "                pbar.update(1)\n",
    "                pbar.set_postfix_str(f\"Last processed: {doc['doc_id']}\")\n",
    "\n",
    "    # Обработка обучающих и тестовых данных\n",
    "    _process_subset(train_data, \"train\")\n",
    "    _process_subset(test_data, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ff29f0a1-4b6d-430b-b5fa-4c68a5683c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train subset: 100%|███████████████████████████| 120000/120000 [27:28<00:00, 72.81it/s, Last processed: 9999]\n",
      "Processing test subset: 100%|█████████████████████████████████| 7600/7600 [01:36<00:00, 78.87it/s, Last processed: 998]\n"
     ]
    }
   ],
   "source": [
    "vectorize_and_save_dataset(model, train_data, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d259c3-9785-4d03-bcd8-39e5de1d4f9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
