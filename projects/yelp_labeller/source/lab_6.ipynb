{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install chromadb==0.5.11\n",
        "!pip install sentence-transformers==3.1.1\n",
        "!pip install evaluate bert_score"
      ],
      "metadata": {
        "id": "sTT-Dx8R-LpL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from chromadb.config import Settings\n",
        "from evaluate import load\n",
        "from typing import Any\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "tcb0TE2y9o0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statistics\n",
        "import pinecone\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "qlQsKNbjCsf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "WE59fmOoBwyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Лабораторная работа №6"
      ],
      "metadata": {
        "id": "wYNkw_TjVs3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Declaring constant"
      ],
      "metadata": {
        "id": "0yyAj72M9pSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры конфигурации для векторного поиска и разделения текста\n",
        "INDEX_NAME = \"VDB\"  # Название индекса для хранения векторных представлений\n",
        "EMBEDDINGS = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'  # Название модели эмбеддингов, используемой для векторизации текстов\n",
        "SIZE = 250  # Размер фрагмента текста для разделения документов\n",
        "OVERLAP = 50  # Перекрытие между фрагментами текста для обеспечения контекста\n",
        "QA_MODEL_NAME = \"deepset/roberta-base-squad2\""
      ],
      "metadata": {
        "id": "_kofnU15EBEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "qa_model = pipeline('question-answering', model=QA_MODEL_NAME, tokenizer=QA_MODEL_NAME, device=\"cuda\")"
      ],
      "metadata": {
        "id": "NFAHpSAG1l8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loader"
      ],
      "metadata": {
        "id": "HKhOsNCaC-ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import re\n",
        "\n",
        "pattern = r\"{price_pattern}|{abbr_patterns}|({phone_pattern})|({email_pattern})|(\\'?[\\w\\-]+)|([^A-Za-z0-9 \\n])\"\n",
        "sent_pattern = r\"((?<=\\.|\\?|!|\\;))({abbr_patterns})\\s\"\n",
        "\n",
        "phone_pattern = r\"\\+?[0-9] ?\\(?[0-9]+\\)?[0-9 -]+\"\n",
        "# [\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}\n",
        "email_pattern = r\"[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+\"\n",
        "price_pattern = r\"(\\$ ?\\d*\\.?\\d+)|(\\d*\\.?\\d+ ?\\$)\"\n",
        "\n",
        "english_abbr = [\"Mr.\", \"Mrs.\", \"Mss.\", \"Ms.\", \"Dr.\"]\n",
        "english_abbr = [x.replace(\".\", \"\\.\") for x in english_abbr]\n",
        "english_abbr.extend(map(lambda x: x.lower(), english_abbr.copy()))\n",
        "\n",
        "sent_pattern = sent_pattern.format(abbr_patterns=\"\".join(map(lambda x: fr\"(?<!{x})\", english_abbr)))\n",
        "sent_pattern = re.compile(sent_pattern)\n",
        "\n",
        "def split_to_sentence(text: str) -> list[str]:\n",
        "    return list(filter(lambda x: len(x) if x else False, sent_pattern.split(text)))"
      ],
      "metadata": {
        "id": "wXqqpd487wpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(split_type=\"train\", n: int | None = None, dataset_path = \"../../assets/{split_type}.csv\", random_state=42) -> pd.DataFrame:\n",
        "    assert split_type == \"train\" or split_type == \"test\"\n",
        "    dataset_path = dataset_path.format(split_type=split_type)\n",
        "    if not os.path.exists(dataset_path):\n",
        "        splits = {'train': 'yelp_review_full/train-00000-of-00001.parquet',\n",
        "                  'test': 'yelp_review_full/test-00000-of-00001.parquet'}\n",
        "        df = pd.read_parquet(\"hf://datasets/Yelp/yelp_review_full/\" + splits[split_type])\n",
        "        df.to_csv(dataset_path, index=False)\n",
        "    else:\n",
        "        df = pd.read_csv(dataset_path)\n",
        "    if n is None:\n",
        "        return df\n",
        "    else:\n",
        "        return train_test_split(df, train_size=n, stratify=df[\"label\"], random_state=random_state)[0]"
      ],
      "metadata": {
        "id": "YBz_FQ8e0viY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_df(df: pd.DataFrame) -> list[tuple[list[str], str|int]]:\n",
        "    data = []\n",
        "    meta = []\n",
        "    ids = []\n",
        "    for idx, row in df.iterrows():\n",
        "        label, text = row[\"label\"], row[\"text\"]\n",
        "        chunks = splitter.split_document(text)\n",
        "        data.extend(chunks)\n",
        "        meta.extend([{\"label\": label} for _ in range(len(chunks))])\n",
        "        ids.extend([f\"{idx}_{i}\" for i in range(len(chunks))])\n",
        "    return data, meta, ids\n",
        "\n",
        "def dataset_batch_iter(df, batch_size):\n",
        "    for df_b in np.array_split(df, batch_size):\n",
        "        yield process_df(df_b)\n",
        "    return"
      ],
      "metadata": {
        "id": "OcLnTqX6548x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Класс для загрузки документов из различных источников, поддерживающий работу с разными форматами файлов\n",
        "class Loader:\n",
        "    def load_single_document(self, file_path: str):\n",
        "        return\n",
        "\n",
        "    def load_documents(self, source_dir: str):\n",
        "        pass  # Метод для загрузки всех документов из указанной директории"
      ],
      "metadata": {
        "id": "Wuqy7ZApCS1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitter"
      ],
      "metadata": {
        "id": "HDGCH3WV_A8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Класс для разделения документов на фрагменты определённого размера с заданным перекрытием\n",
        "class Splitter:\n",
        "    def __init__(self, chunk_size, chunk_overlap):\n",
        "        assert chunk_size > chunk_overlap\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap=chunk_overlap\n",
        "\n",
        "    def split_document(self, document: str):\n",
        "        # Метод для разделения переданных документов на фрагменты\n",
        "        doc_sents = []\n",
        "        for sent in split_to_sentence(document):\n",
        "          for i in range(0, len(sent), self.chunk_size-self.chunk_overlap):\n",
        "            start, end = i, i+self.chunk_size\n",
        "            doc_sents.append(sent[start: end])\n",
        "        return doc_sents"
      ],
      "metadata": {
        "id": "CvN9kIBmCyDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter=Splitter(SIZE, OVERLAP)"
      ],
      "metadata": {
        "id": "W3Uerv4k8CId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitter.split_document(\"I love driving and I dont loke kaksd asdkf\")"
      ],
      "metadata": {
        "id": "2vYGRuYI9Li6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"./assets\"):\n",
        "  os.mkdir(\"./assets\")"
      ],
      "metadata": {
        "id": "GddIEwf36gMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(split_type=\"train\", n=10, dataset_path='./assets/{split_type}.csv')\n",
        "for i in dataset_batch_iter(df, batch_size = 2):\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "id": "Ci6CrXH78NTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vector database"
      ],
      "metadata": {
        "id": "ohzsSpJX9u3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Базовый класс для работы с коллекцией документов, поддерживающий добавление, поиск и очистку данных\n",
        "class Collector:\n",
        "    def add(self, texts: list[str], metadatas: list[dict]):\n",
        "        pass  # Метод для добавления текстов и связанных с ними метаданных в коллекцию\n",
        "\n",
        "    def add_from_directory(self, dir_path: str):\n",
        "        pass  # Метод для добавления документов в коллекцию из указанной директории\n",
        "\n",
        "    def get(self, search_strings: list[str], n_results: int) -> list[Document]:\n",
        "        pass  # Метод для поиска документов по строкам запроса с ограничением на количество результатов\n",
        "\n",
        "    def get_documents(self, search_string: str, n_results: int, score_threshold: float) -> list[Document]:\n",
        "        pass  # Метод для поиска документов с учётом порога релевантности и количества возвращаемых результатов\n",
        "\n",
        "    def clear(self):\n",
        "        pass  # Метод для очистки коллекции документов"
      ],
      "metadata": {
        "id": "dn9FQiJd-8TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Базовый класс для создания эмбеддингов, обеспечивающий интерфейс для получения модели эмбеддингов\n",
        "class Embedder:\n",
        "    def __init__(self, model_name):\n",
        "        pass  # Инициализация эмбеддера\n",
        "\n",
        "    def get_embedding(self, sent):\n",
        "        pass  # Метод для получения модели эмбеддингов, которая будет использоваться для векторизации текстов"
      ],
      "metadata": {
        "id": "soiFkrEC_kPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEmbedder(Embedder):\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"):\n",
        "       self.model = SentenceTransformer(model_name)\n",
        "    def get_embedding(self, sent):\n",
        "        # Метод для получения модели эмбеддингов, которая будет использоваться для векторизации текстов\n",
        "        return self.model.encode(sent).tolist()\n",
        "    def __call__(self, input):\n",
        "        return self.get_embedding(input)"
      ],
      "metadata": {
        "id": "grQtAn0VDPqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChromaCollector(Collector):\n",
        "    def __init__(self, name_prefix, root_path, embeddnig_fn, distance_fn):\n",
        "      self.client = chromadb.PersistentClient(path=root_path)\n",
        "      self.distance_fn = distance_fn\n",
        "      self.embedding_fn = embeddnig_fn\n",
        "      self._collection_name = name_prefix + self.distance_fn\n",
        "      self.database = self.get_database()\n",
        "\n",
        "    def get_database(self):\n",
        "      return self.client.get_or_create_collection(\n",
        "            self._collection_name,\n",
        "            metadata={\"hnsw:space\": self.distance_fn},\n",
        "            embedding_function=self.embedding_fn\n",
        "        )\n",
        "\n",
        "    def load_dataset(self, df: pd.DataFrame, batch_size=128) -> None:\n",
        "        for chunks, metas, ids in tqdm(dataset_batch_iter(df, batch_size = batch_size), total=math.ceil(df.shape[0] / batch_size), desc=\"loading to the DB\"):\n",
        "          self.database.add(\n",
        "                documents=chunks,\n",
        "                metadatas=metas,\n",
        "                ids=ids\n",
        "            )\n",
        "\n",
        "    def query(self, query, n_results: int, query_texts=None, where=None, where_document=None):\n",
        "        return self.database.query(\n",
        "            n_results=n_results,\n",
        "            query_texts=query_texts,\n",
        "            query_embeddings=self.embedding_fn(query),\n",
        "            where=where,\n",
        "            where_document=where_document\n",
        "        )\n",
        "\n",
        "    def clear(self):\n",
        "        self.client.delete_collection(self._collection_name)"
      ],
      "metadata": {
        "id": "C9T7ta-q_qDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implementation vector database"
      ],
      "metadata": {
        "id": "6XXhGdIuDmUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_index = '/VDB' #@param {type:\"string\"}\n",
        "path_to_df = './assets/{split_type}.csv' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "h8xU0gGbERSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import math"
      ],
      "metadata": {
        "id": "uTOUXhr9CeMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(split_type=\"train\", n=10_000, dataset_path=path_to_df)\n",
        "emedder = SentenceEmbedder()"
      ],
      "metadata": {
        "id": "v631_WnyDMUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database_cos = ChromaCollector(\"my_db\", path_to_index, emedder, \"cosine\")"
      ],
      "metadata": {
        "id": "v0M9W55lCi3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database_cos.load_dataset(df)"
      ],
      "metadata": {
        "id": "pJw93tPsCE-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Search"
      ],
      "metadata": {
        "id": "EP4n4OAZ_VuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What is your favorite food?' #@param {type:\"string\"}\n",
        "n_results = 5 #@param {type:\"integer\"}\n",
        "# score_threshold = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "result = database_cos.query(query, n_results=5)\n",
        "for dist, document, meta in zip(result[\"distances\"][0], result[\"documents\"][0], result[\"metadatas\"][0]):\n",
        "  print(f\"{dist:0.2f}  {meta['label']}  {document}\")"
      ],
      "metadata": {
        "id": "E6s-niuuIlqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "m6lzqXLZ_YtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_top = 10 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "VFNalqWBkF-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [ # text_query, doc_id\n",
        "    (\"What color is a parking pass?\", 121392),\n",
        "    (\"Does the discount compensate the workers incompetent?\", 340905),\n",
        "    (\"How long I was a Centurylink customer?\",561808),\n",
        "    (\"What was a spicy level for the Panang Curry\", 153961),\n",
        "    (\"How many times does the servers came back?\", 596493),\n",
        "    (\"Where is a good Ted Wien's store located?\", 257236),\n",
        "    (\"Which DJ played on Paul Oakenfield night?\", 213914),\n",
        "    (\"How many stars does the Carvel had last year?\", 418978),\n",
        "    (\"What item did a man from Arizona buy two at a time?\", 33736),\n",
        "    (\"What dish\\drink was cancelled because of its taste?\", 335445)\n",
        "]\n",
        "answers = [\n",
        "    'white',\n",
        "    \"A discount doesn't compensate\",\n",
        "    '18 months',\n",
        "    '8 spicy level',\n",
        "    'twice',\n",
        "    'Blue Diamond and Arville',\n",
        "    'David Guetta',\n",
        "    'a solid 4.',\n",
        "    \"shirts\",\n",
        "    \"the latte\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "CkRfl23C96ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.index.isin(list(map(lambda x: x[1], queries)))].iloc[:, :]"
      ],
      "metadata": {
        "id": "BUH461ub88Oj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from evaluate import load\n",
        "bertscore = load(\"bertscore\")"
      ],
      "metadata": {
        "id": "261rmtCrBgg-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_context(query, n_results=10):\n",
        "  return database_cos.query(query=query, n_results=n_results)"
      ],
      "metadata": {
        "id": "4rtNMGqhBMEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_scores = []\n",
        "for querу, answer, idx in zip(queries, answers, range(len(answers))):\n",
        "  question, idx = querу\n",
        "  context = get_context(question, n_top)\n",
        "  qa_input = {'question': question,\n",
        "             'context': ' '.join(context[\"documents\"][0])}\n",
        "  res = qa_model(qa_input)\n",
        "  bs = bertscore.compute(predictions=[res['answer']], references=[answer], lang=\"en\")\n",
        "  bert_scores.append(bs)\n",
        "\n",
        "  print(f'Question: {question}\\nAnswer: {res[\"answer\"]}\\nUser answer: {answer}\\nScore: {bs[\"f1\"][0]}\\n ')"
      ],
      "metadata": {
        "id": "KVcRQPcP2vMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bert_score = np.array([x[\"f1\"] for x in bert_scores], dtype=\"float64\")\n",
        "my_score = np.array([0, 1, 0, 0, 0, 1, 1, 1, 0, 0])\n",
        "\n",
        "\n",
        "print(\"Средняя оценка BERT F1:\" ,np.mean(bert_score))\n",
        "print(\"Средняя оценка MY:\" ,np.mean(my_score))"
      ],
      "metadata": {
        "id": "89Sizdjw-HpF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def echo(message, history):\n",
        "  context = get_context(message, n_top)\n",
        "  qa_input = {'question': message,\n",
        "             'context': ' '.join(context[\"documents\"][0])}\n",
        "  res = qa_model(qa_input)\n",
        "  return res['answer']\n",
        "\n",
        "demo = gr.ChatInterface(fn=echo, examples=[\"hello\"], title=\"My shiny Bot\")\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "qCWklbCypUqZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}