{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate==0.4.3\n",
        "!pip install llama-cpp-python==0.1.9\n",
        "!pip install pinecone-client==5.0.1\n",
        "!pip install langchain_community==0.2.16\n",
        "!pip install langchain-chroma==0.1.4\n",
        "!pip install chromadb==0.5.11\n",
        "!pip install sentence-transformers==3.1.1"
      ],
      "metadata": {
        "id": "sTT-Dx8R-LpL",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PDFMinerLoader, TextLoader, CSVLoader, UnstructuredWordDocumentLoader, UnstructuredHTMLLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from langchain_chroma.vectorstores import Chroma\n",
        "from langchain.schema import Document\n",
        "from chromadb.config import Settings\n",
        "from llama_cpp import Llama\n",
        "from evaluate import load\n",
        "from typing import Any\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "tcb0TE2y9o0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statistics\n",
        "import pinecone\n",
        "import glob\n",
        "import os"
      ],
      "metadata": {
        "id": "qlQsKNbjCsf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ],
      "metadata": {
        "id": "WE59fmOoBwyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Лабораторная работа №5"
      ],
      "metadata": {
        "id": "wYNkw_TjVs3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Declaring constant"
      ],
      "metadata": {
        "id": "0yyAj72M9pSU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyaoDfoC9bmT"
      },
      "outputs": [],
      "source": [
        "# Словарь, сопоставляющий расширения файлов с соответствующими загрузчиками данных и их параметрами\n",
        "LOADER_MAPPING = {\n",
        "    \".csv\": (CSVLoader, {}),\n",
        "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
        "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
        "    \".html\": (UnstructuredHTMLLoader, {}),\n",
        "    \".pdf\": (PDFMinerLoader, {}),\n",
        "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Параметры конфигурации для векторного поиска и разделения текста\n",
        "INDEX_NAME = \"VDB\"  # Название индекса для хранения векторных представлений\n",
        "EMBEDDINGS = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'  # Название модели эмбеддингов, используемой для векторизации текстов\n",
        "SIZE = 250  # Размер фрагмента текста для разделения документов\n",
        "OVERLAP = 50  # Перекрытие между фрагментами текста для обеспечения контекста"
      ],
      "metadata": {
        "id": "_kofnU15EBEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Loader"
      ],
      "metadata": {
        "id": "HKhOsNCaC-ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import re\n",
        "\n",
        "pattern = r\"{price_pattern}|{abbr_patterns}|({phone_pattern})|({email_pattern})|(\\'?[\\w\\-]+)|([^A-Za-z0-9 \\n])\"\n",
        "sent_pattern = r\"((?<=\\.|\\?|!|\\;))({abbr_patterns})\\s\"\n",
        "\n",
        "phone_pattern = r\"\\+?[0-9] ?\\(?[0-9]+\\)?[0-9 -]+\"\n",
        "# [\\+]?[(]?[0-9]{3}[)]?[-\\s\\.]?[0-9]{3}[-\\s\\.]?[0-9]{4,6}\n",
        "email_pattern = r\"[^@ \\t\\r\\n]+@[^@ \\t\\r\\n]+\\.[^@ \\t\\r\\n]+\"\n",
        "price_pattern = r\"(\\$ ?\\d*\\.?\\d+)|(\\d*\\.?\\d+ ?\\$)\"\n",
        "\n",
        "english_abbr = [\"Mr.\", \"Mrs.\", \"Mss.\", \"Ms.\", \"Dr.\"]\n",
        "english_abbr = [x.replace(\".\", \"\\.\") for x in english_abbr]\n",
        "english_abbr.extend(map(lambda x: x.lower(), english_abbr.copy()))\n",
        "\n",
        "sent_pattern = sent_pattern.format(abbr_patterns=\"\".join(map(lambda x: fr\"(?<!{x})\", english_abbr)))\n",
        "sent_pattern = re.compile(sent_pattern)\n",
        "\n",
        "def split_to_sentence(text: str) -> list[str]:\n",
        "    return list(filter(lambda x: len(x) if x else False, sent_pattern.split(text)))"
      ],
      "metadata": {
        "id": "wXqqpd487wpJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(split_type=\"train\", n: int | None = None, dataset_path = \"../../assets/{split_type}.csv\", random_state=42) -> pd.DataFrame:\n",
        "    assert split_type == \"train\" or split_type == \"test\"\n",
        "    dataset_path = dataset_path.format(split_type=split_type)\n",
        "    if not os.path.exists(dataset_path):\n",
        "        splits = {'train': 'yelp_review_full/train-00000-of-00001.parquet',\n",
        "                  'test': 'yelp_review_full/test-00000-of-00001.parquet'}\n",
        "        df = pd.read_parquet(\"hf://datasets/Yelp/yelp_review_full/\" + splits[split_type])\n",
        "        df.to_csv(dataset_path, index=False)\n",
        "    else:\n",
        "        df = pd.read_csv(dataset_path)\n",
        "    if n is None:\n",
        "        return df\n",
        "    else:\n",
        "        return train_test_split(df, train_size=n, stratify=df[\"label\"], random_state=random_state)[0]"
      ],
      "metadata": {
        "id": "YBz_FQ8e0viY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_df(df: pd.DataFrame) -> list[tuple[list[str], str|int]]:\n",
        "    data = []\n",
        "    meta = []\n",
        "    ids = []\n",
        "    for idx, row in df.iterrows():\n",
        "        label, text = row[\"label\"], row[\"text\"]\n",
        "        chunks = splitter.split_document(text)\n",
        "        data.extend(chunks)\n",
        "        meta.extend([{\"label\": label} for _ in range(len(chunks))])\n",
        "        ids.extend([f\"{idx}_{i}\" for i in range(len(chunks))])\n",
        "    return data, meta, ids\n",
        "\n",
        "def dataset_batch_iter(df, batch_size):\n",
        "    for df_b in np.array_split(df, batch_size):\n",
        "        yield process_df(df_b)\n",
        "    return"
      ],
      "metadata": {
        "id": "OcLnTqX6548x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Класс для загрузки документов из различных источников, поддерживающий работу с разными форматами файлов\n",
        "class Loader:\n",
        "    def load_single_document(self, file_path: str):\n",
        "        return\n",
        "\n",
        "    def load_documents(self, source_dir: str):\n",
        "        pass  # Метод для загрузки всех документов из указанной директории"
      ],
      "metadata": {
        "id": "Wuqy7ZApCS1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Splitter"
      ],
      "metadata": {
        "id": "HDGCH3WV_A8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Класс для разделения документов на фрагменты определённого размера с заданным перекрытием\n",
        "class Splitter:\n",
        "    def __init__(self, chunk_size, chunk_overlap):\n",
        "        assert chunk_size > chunk_overlap\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap=chunk_overlap\n",
        "\n",
        "    def split_document(self, document: str):\n",
        "        # Метод для разделения переданных документов на фрагменты\n",
        "        doc_sents = []\n",
        "        for sent in split_to_sentence(document):\n",
        "          for i in range(0, len(sent), self.chunk_size-self.chunk_overlap):\n",
        "            start, end = i, i+self.chunk_size\n",
        "            doc_sents.append(sent[start: end])\n",
        "        return doc_sents"
      ],
      "metadata": {
        "id": "CvN9kIBmCyDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "splitter=Splitter(SIZE, OVERLAP)"
      ],
      "metadata": {
        "id": "W3Uerv4k8CId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitter.split_document(\"I love driving and I dont loke kaksd asdkf\")"
      ],
      "metadata": {
        "id": "2vYGRuYI9Li6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(split_type=\"train\", n=10, dataset_path='./assets/{split_type}.csv')\n",
        "for i in dataset_batch_iter(df, batch_size = 2):\n",
        "  print(i)\n",
        "  break"
      ],
      "metadata": {
        "id": "Ci6CrXH78NTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Vector database"
      ],
      "metadata": {
        "id": "ohzsSpJX9u3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Базовый класс для работы с коллекцией документов, поддерживающий добавление, поиск и очистку данных\n",
        "class Collector:\n",
        "    def add(self, texts: list[str], metadatas: list[dict]):\n",
        "        pass  # Метод для добавления текстов и связанных с ними метаданных в коллекцию\n",
        "\n",
        "    def add_from_directory(self, dir_path: str):\n",
        "        pass  # Метод для добавления документов в коллекцию из указанной директории\n",
        "\n",
        "    def get(self, search_strings: list[str], n_results: int) -> list[Document]:\n",
        "        pass  # Метод для поиска документов по строкам запроса с ограничением на количество результатов\n",
        "\n",
        "    def get_documents(self, search_string: str, n_results: int, score_threshold: float) -> list[Document]:\n",
        "        pass  # Метод для поиска документов с учётом порога релевантности и количества возвращаемых результатов\n",
        "\n",
        "    def clear(self):\n",
        "        pass  # Метод для очистки коллекции документов"
      ],
      "metadata": {
        "id": "dn9FQiJd-8TP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Базовый класс для создания эмбеддингов, обеспечивающий интерфейс для получения модели эмбеддингов\n",
        "class Embedder:\n",
        "    def __init__(self, model_name):\n",
        "        pass  # Инициализация эмбеддера\n",
        "\n",
        "    def get_embedding(self, sent):\n",
        "        pass  # Метод для получения модели эмбеддингов, которая будет использоваться для векторизации текстов"
      ],
      "metadata": {
        "id": "soiFkrEC_kPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceEmbedder(Embedder):\n",
        "    def __init__(self, model_name: str = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"):\n",
        "       self.model = SentenceTransformer(model_name)\n",
        "    def get_embedding(self, sent):\n",
        "        # Метод для получения модели эмбеддингов, которая будет использоваться для векторизации текстов\n",
        "        return self.model.encode(sent).tolist()\n",
        "    def __call__(self, input):\n",
        "        return self.get_embedding(input)"
      ],
      "metadata": {
        "id": "grQtAn0VDPqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ChromaCollector(Collector):\n",
        "    def __init__(self, name_prefix, root_path, embeddnig_fn, distance_fn):\n",
        "      self.client = chromadb.PersistentClient(path=root_path)\n",
        "      self.distance_fn = distance_fn\n",
        "      self.embedding_fn = embeddnig_fn\n",
        "      self._collection_name = name_prefix + self.distance_fn\n",
        "      self.database = self.get_database()\n",
        "\n",
        "    def get_database(self):\n",
        "      return self.client.get_or_create_collection(\n",
        "            self._collection_name,\n",
        "            metadata={\"hnsw:space\": self.distance_fn},\n",
        "            embedding_function=self.embedding_fn\n",
        "        )\n",
        "\n",
        "    def load_dataset(self, df: pd.DataFrame, batch_size=128) -> None:\n",
        "        for chunks, metas, ids in tqdm(dataset_batch_iter(df, batch_size = batch_size), total=math.ceil(df.shape[0] / batch_size), desc=\"loading to the DB\"):\n",
        "          self.database.add(\n",
        "                documents=chunks,\n",
        "                metadatas=metas,\n",
        "                ids=ids\n",
        "            )\n",
        "\n",
        "    def query(self, query, n_results: int, query_texts=None, where=None, where_document=None):\n",
        "        return self.database.query(\n",
        "            n_results=n_results,\n",
        "            query_texts=query_texts,\n",
        "            query_embeddings=self.embedding_fn(query),\n",
        "            where=where,\n",
        "            where_document=where_document\n",
        "        )\n",
        "\n",
        "    def clear(self):\n",
        "        self.client.delete_collection(self._collection_name)"
      ],
      "metadata": {
        "id": "C9T7ta-q_qDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Implementation vector database"
      ],
      "metadata": {
        "id": "6XXhGdIuDmUU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_index = '/VDB' #@param {type:\"string\"}\n",
        "path_to_df = './assets/{split_type}.csv' #@param {type:\"string\"}"
      ],
      "metadata": {
        "id": "h8xU0gGbERSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "import math"
      ],
      "metadata": {
        "id": "uTOUXhr9CeMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(split_type=\"train\", n=10_000, dataset_path=path_to_df)\n",
        "emedder = SentenceEmbedder()"
      ],
      "metadata": {
        "id": "v631_WnyDMUw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database_cos = ChromaCollector(\"my_db\", path_to_index, emedder, \"cosine\")"
      ],
      "metadata": {
        "id": "v0M9W55lCi3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "database_cos.load_dataset(df)"
      ],
      "metadata": {
        "id": "pJw93tPsCE-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Search"
      ],
      "metadata": {
        "id": "EP4n4OAZ_VuG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What is your favorite food?' #@param {type:\"string\"}\n",
        "n_results = 5 #@param {type:\"integer\"}\n",
        "# score_threshold = 0.5 # @param {type:\"slider\", min:0, max:1, step:0.1}\n",
        "\n",
        "result = database_cos.query(query, n_results=5)\n",
        "for dist, document, meta in zip(result[\"distances\"][0], result[\"documents\"][0], result[\"metadatas\"][0]):\n",
        "  print(f\"{dist:0.2f}  {meta['label']}  {document}\")"
      ],
      "metadata": {
        "id": "E6s-niuuIlqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What is your favorite restaurant?' #@param {type:\"string\"}\n",
        "n_results = 5 #@param {type:\"integer\"}\n",
        "\n",
        "result = database_cos.query(query, n_results=5)\n",
        "for dist, document, meta in zip(result[\"distances\"][0], result[\"documents\"][0], result[\"metadatas\"][0]):\n",
        "  print(f\"{dist:0.2f}  {meta['label']}  {document}\")"
      ],
      "metadata": {
        "id": "traAdX2tu3_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What do you think about this doctor?' #@param {type:\"string\"}\n",
        "n_results = 5 #@param {type:\"integer\"}\n",
        "\n",
        "result = database_cos.query(query, n_results=5)\n",
        "for dist, document, meta in zip(result[\"distances\"][0], result[\"documents\"][0], result[\"metadatas\"][0]):\n",
        "  print(f\"{dist:0.2f}  {meta['label']}  {document}\")"
      ],
      "metadata": {
        "id": "IK4S89GBu4TV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Evaluation"
      ],
      "metadata": {
        "id": "m6lzqXLZ_YtU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Класс для оценки работы коллектора, предоставляющий функционал для поиска, оценки и расчета статистики по результатам\n",
        "class CollectorEvaluator:\n",
        "    def __init__(self, collector: Collector, n_top=100):\n",
        "        pass  # Инициализация коллектора и параметра n_top для ограничения числа возвращаемых результатов\n",
        "\n",
        "    def explore_collector(self, text):\n",
        "        pass  # Метод для поиска документов в коллекторе на основе текста запроса\n",
        "\n",
        "    def eval(self, query, answer):\n",
        "        pass  # Метод для оценки корректности найденных документов на основе запроса и правильного ответа\n",
        "\n",
        "    def calculate_statistics(self, data):\n",
        "        pass  # Метод для расчета статистических показателей (например, минимальное, максимальное, среднее значение)\n",
        "\n",
        "    def explore_and_calculate(self, data):\n",
        "        pass  # Метод для проведения поиска по данным и расчета статистики на основе результатов"
      ],
      "metadata": {
        "id": "WHZ6IMLIQq2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_docs = 100 #@param {type:\"integer\"}\n",
        "n_top = 100 #@param {type:\"integer\"}"
      ],
      "metadata": {
        "id": "VFNalqWBkF-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_qa = df.iloc[:n_docs,]"
      ],
      "metadata": {
        "id": "wW7cLRBlzeg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_id_index(result_ids, my_id, trunc=False):\n",
        "  for idx, rid in enumerate(result_ids):\n",
        "    if trunc:\n",
        "      if rid.split(\"_\")[0]==my_id.split(\"_\")[0]:\n",
        "        return idx\n",
        "    else:\n",
        "      if rid == my_id:\n",
        "        return idx\n",
        "  return"
      ],
      "metadata": {
        "id": "-B8gWN1XznlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_id_index([\"111\", \"123\"], \"123\", trunc=True)"
      ],
      "metadata": {
        "id": "nYLFzsGL9kFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_id_indexes = []\n",
        "for chunks, metas, ids in tqdm(dataset_batch_iter(df_qa, batch_size = 1), desc=\"qa DB\"):\n",
        "  for chunk, meta, id_ in zip(chunks, metas, ids):\n",
        "    results = database_cos.query(query=chunk, n_results=n_top)\n",
        "    ch_id = find_id_index(results[\"ids\"][0], id_, trunc=False)\n",
        "    find_id_indexes.append(ch_id)"
      ],
      "metadata": {
        "id": "nPQolm4Wza9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = np.array(find_id_indexes, dtype=\"float64\")\n",
        "\n",
        "print(\"Средняя позиция документа:\" ,np.mean(indexes[~np.isnan(indexes)]))\n",
        "print(\"Количество не найденных документов:\", np.count_nonzero(np.isnan(indexes)))"
      ],
      "metadata": {
        "id": "wYyAmInz0IqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = [ # text_query, doc_id\n",
        "    (\"What color is a parking pass?\", 121392),\n",
        "    (\"Does the discount compensate the workers incompetent?\", 340905),\n",
        "    (\"How long I was a Centurylink customer?\",561808),\n",
        "    (\"What was a spicy level for the Panang Curry\", 153961),\n",
        "    (\"How many times does the servers came back?\", 596493),\n",
        "    (\"Where is a good Ted Wien's store located?\", 257236),\n",
        "    (\"Which DJ played on Paul Oakenfield night?\", 213914),\n",
        "    (\"How many stars does the Carvel had last year?\", 418978),\n",
        "    (\"What item did a man from Arizona buy two at a time?\", 33736),\n",
        "    (\"What dish\\drink was cancelled because of its taste?\", 335445)\n",
        "]"
      ],
      "metadata": {
        "id": "CkRfl23C96ow"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_id_indexes = []\n",
        "for data in tqdm(queries):\n",
        "  query, id_ = data\n",
        "  results = database_cos.query(query=query, n_results=n_top)\n",
        "  ch_id = find_id_index(results[\"ids\"][0], str(id_), trunc=True)\n",
        "  find_id_indexes.append(ch_id)"
      ],
      "metadata": {
        "id": "6AmKHGwO9YZk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "indexes = np.array(find_id_indexes, dtype=\"float64\")\n",
        "\n",
        "print(\"Средняя позиция документа:\" ,np.mean(indexes[~np.isnan(indexes)]))\n",
        "print(\"Количество не найденных документов:\", np.count_nonzero(np.isnan(indexes)))"
      ],
      "metadata": {
        "id": "89Sizdjw-HpF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}