{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def tokenize(text):\n",
    "    matches = []\n",
    "    cleaned_tokens = []\n",
    "    #Удаляем email\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "\n",
    "    # Удаляем пунктуацию\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Удаляем телефонные номера\n",
    "    text = re.sub(r\"^\\\\+?[1-9][0-9]{7,14}$\", '', text)\n",
    "    \n",
    "    new_sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|\\n)\\s', text)\n",
    "\n",
    "\n",
    "    for sentences in new_sentences:\n",
    "        for line in sentences.split(r\"(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|!|\\n)\\s\"):\n",
    "            clear_token = re.findall(r'\\b\\w+\\b|[\\(\\),.—:;!?|<>\"]', line)\n",
    "            if clear_token:\n",
    "                cleaned_tokens.append(clear_token)\n",
    "\n",
    "    for sentence in cleaned_tokens:\n",
    "        for num, token in enumerate(sentence):\n",
    "            for i, match in enumerate(matches):\n",
    "                if token == f'__regex_match_{i}__':\n",
    "                    sentence[num] = match\n",
    "    return cleaned_tokens\n",
    "\n",
    "def processing_sentences(sentences):\n",
    "    tokens = [tokenize(sentence) for sentence in sentences]\n",
    "    return tokens\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def stemma(tokens_list):\n",
    "    stemmed_tokens_list = list()\n",
    "    for tokens in tokens_list:\n",
    "        stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "        stemmed_tokens_list.append(stemmed_tokens)\n",
    "    return stemmed_tokens_list"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def lemma(tokens_list):\n",
    "    lemmatized_tokens_list = list()\n",
    "    for tokens in tokens_list:\n",
    "        lemmatized_tokens = [lemmatizer.lemmatize(token.lower()) for token in tokens]\n",
    "        lemmatized_tokens_list.append(lemmatized_tokens)\n",
    "    return lemmatized_tokens_list\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def save_tsv_file(tokens, stemmed_words, lemmatized_words, output_path, output_filename):\n",
    "    output = f'{output_path}/{output_filename}.tsv'\n",
    "    with open(output, 'w', encoding='utf-8') as output_file:\n",
    "        for token_sentence, stemmed_sentence, lemmatized_sentence in zip(tokens, stemmed_words, lemmatized_words):\n",
    "            for token, stemma, lemma in zip(token_sentence, stemmed_sentence, lemmatized_sentence):\n",
    "                  output_file.write(f\"{token}\\t{stemma}\\t{lemma}\\n\")\n",
    "            output_file.write(f\"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "def processing_files(category_path, file, category_path_new):\n",
    "        sample_file_path = os.path.join(category_path, file)\n",
    "        with open(sample_file_path, 'r', encoding='latin1') as file_name:\n",
    "            sample_content = file_name.read()\n",
    "        tokens = tokenize(sample_content)\n",
    "        lemmatized_words = lemma(tokens)\n",
    "        stemmed_words = stemma(tokens)\n",
    "\n",
    "        save_tsv_file(tokens, stemmed_words, lemmatized_words, category_path_new, file)\n",
    "\n",
    "def processing_other_folders(category_path, folder, file_, folder_new):\n",
    "    files_category = os.listdir(category_path)\n",
    "    folder_new = os.path.join(folder_new, file_)\n",
    "    os.makedirs(f'{folder_new}')\n",
    "    for file in tqdm(files_category, desc=f'Folder: {folder} | Class: {file_}'):\n",
    "        if os.path.isdir(os.path.join(category_path, file)):\n",
    "            print(os.path.join(category_path, file))\n",
    "            processing_other_folders(os.path.join(category_path, file), folder, file, folder_new)\n",
    "        if os.path.isfile(os.path.join(category_path, file)):\n",
    "            processing_files(category_path, file, folder_new)\n",
    "\n",
    "def processing_main_folders(folder, folder_new):\n",
    "    folder_category = os.listdir(folder)\n",
    "    for category in folder_category:\n",
    "        category_path = os.path.join(folder, category)\n",
    "        category_path_new = os.path.join(folder_new, category)\n",
    "        os.makedirs(f'{category_path_new}')\n",
    "        files_category = os.listdir(category_path)\n",
    "        for file in tqdm(files_category, desc=f'Folder: {folder} | Class: {category}'):\n",
    "            if os.path.isfile(os.path.join(category_path, file)):\n",
    "                processing_files(category_path, file, category_path_new)\n",
    "            elif os.path.isdir(os.path.join(category_path, file)):\n",
    "                processing_other_folders(os.path.join(category_path, file), folder, file, category_path_new)\n",
    "\n",
    "\n",
    "def processing(folders):\n",
    "    for folder in folders:\n",
    "        folder_new = folder.split('-')[-1]\n",
    "        processing_main_folders(folder, folder_new)\n",
    "        \n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "folders = ['D:/agorcueva/ITMO/NLP/20news-bydate-train', 'D:/agorcueva/ITMO/NLP/20news-bydate-test']\n",
    "processing(folders)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "os.listdir('D:/agorcueva/ITMO/NLP/20news-bydate-train')",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for f in os.listdir(folder_category):\n",
    "    if not f.startswith('.'):\n",
    "        yield f"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
