# -*- coding: utf-8 -*-
"""ChromaDB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZtH0iBMQcKF2AxnrciR8PAH5dudhkJXb
"""

from google.colab import drive
drive.mount('/content/drive')
!ls "/content/drive/"

!pip install evaluate==0.4.3
!pip install llama-cpp-python==0.1.9
!pip install pinecone-client==5.0.1
!pip install langchain_community==0.2.16
!pip install langchain-chroma==0.1.4
!pip install chromadb==0.5.11
!pip install sentence-transformers==3.1.1

from sentence_transformers import SentenceTransformer
import chromadb
import re
import os
import pandas as pd
import csv
from tqdm import tqdm

import string

class Loader:
    def load_single_document(self, file_path: str):
      # deprecated
      # I am noping out of screenplays, lets do synopses instead
      with open(file_path, "r") as f:
        return f.read()

    def load_documents(self, source: str = '/content/drive/My Drive/my_root/nlp_itmo/assets/raw_data/movie_meta_data.csv'):
      """
      returns list, not dataframe, and metadata list of dict
      """
      corpus = []
      meta = []
      meta_df = pd.read_csv(source, index_col='imdbid')
      meta_df = meta_df.dropna(subset=['title', 'directors', 'keywords', 'genres', 'year', 'synopsis'])

      pattern = r'\b\w{1,4}[!.]'

      def remove_short_punctuation(text):
        # Use re.sub to replace matches with just the word part
        return re.sub(pattern, lambda m: m.group()[:-1], text)

      corpus = [remove_short_punctuation(x) for x in meta_df['synopsis']]
      directors = meta_df['directors'].to_list()
      # keywords =  meta_df['keywords'].to_list()
      genres = meta_df['genres'].to_list()
      years = meta_df['year'].to_list()
      titles = meta_df['title'].to_list()

      for i in range(len(corpus)):
        meta.append({
          "directors": directors[i],
          # "keywords": keywords[i],
          "genres": genres[i],
          "years": years[i],
          "titles": titles[i]
        })


      return corpus, meta

l = Loader()
corpus, meta = l.load_documents()
corpus[100]

class Splitter:
    def __init__(self, chunk_size, chunk_overlap):
        self.chunk = chunk_size
        self.overlap = chunk_overlap

    def split_document(self, doc):
      ch = []
      sentences = re.split(
              r"(((?<!\w\.\w.)(?<!\s\w\.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s(?=[A-Z]))|((?<![\,\-\:])\n(?=[A-Z]|\" )))", doc)[::4]
      for sent in sentences:
        for i in range(0, len(sent), self.chunk-self.overlap):
          start = sent.rfind(" ", 0, i)
          if start == -1:
            start = i
          end = sent.find(" ", i+self.chunk)
          ch.append(sent[start: end])
      return ch

    def split_documents(self, documents):
        ch = []
        for doc in documents:
          ch.append(self.split_document(doc))
        return ch

s = Splitter(chunk_size=300, chunk_overlap=50)
s.split_document(corpus[100])

class Collector:
    def add(self, texts: list[str], metadatas: list[dict]):
      pass

    def add_from_directory(self, dir_path: str):
      pass

    def get(self, search_strings: list[str], n_results: int) -> list:
      pass

    def get_documents(self, search_string: str, n_results: int, score_threshold: float) -> list:
      pass

    def clear(self):
      pass

class ChromaCollector(Collector):
  def __init__(self, name, root_path, embeddnig_fn, distance_fn):
    self.client = chromadb.PersistentClient(path=root_path)
    self.distance_fn = distance_fn
    self.embedding_fn = embeddnig_fn
    self.collection = self.client.get_or_create_collection(name=name,
                                                      metadata={"hnsw:space": self.distance_fn},
                                                      embedding_function=self.embedding_fn)

  def add(self, texts: list[str], metadatas: list[dict], ids: list[int]):
    self.collection.add(
      documents=texts,
      metadatas=metadatas,
      ids=ids
    )

  def clear(self):
    self.client.reset()

class Embedder():
   def __init__(self):
    pass

   def encode(self):
    pass


   def __call__(self, input):
    input = self.model.encode(input).tolist()
    return input

class SentenceEmbedder(Embedder):
    def __init__(self, model_name: str = "sentence-transformers/all-MiniLM-L6-v2"):
       self.model = SentenceTransformer(model_name)
    def get_embedding(self, sent):
        # Метод для получения модели эмбеддингов, которая будет использоваться для векторизации текстов
        return self.model.encode(sent).tolist()
    def __call__(self, input):
        return self.get_embedding(input)

# init some stuff
path = "/content/drive/My Drive/my_root/nlp_itmo/assets/db"
embedder = SentenceEmbedder()
database = ChromaCollector("my_db", path, embedder, "cosine")

# i = 100
# doc = s.split_document(corpus[i])
# database.add(
#     texts=doc,
#     metadatas=[meta[i] for _ in range(len(doc))],
#     ids=[meta[i]["titles"]+" "+str(j) for j in range(len(doc))]
# )

# database.collection.query(embedder("Woman is unhappy and struggling with self-esteem issues"))

# fill the db
# for i in range(len(corpus)):
#   doc = s.split_document(corpus[i])
#   database.add(
#       texts=doc,
#       metadatas=[meta[i] for _ in range(len(doc))],
#       ids=[meta[i]["titles"]+" "+str(meta[i]["years"])+" "+str(j) for j in range(len(doc))]
#   )

query = 'Can leutenant Kaffee handle the truth' #@param {type:"string"}
n_results = 5 #@param {type:"integer"}
score_threshold = 0.5 # @param {type:"slider", min:0, max:1, step:0.1}

db = database.collection
query = embedder(query)
#Нужно реализовать эксперимент по поиску в векторном индексе
result = db.query(query, n_results=n_results)
result

# answer position: 1

result = db.query(embedder("The laziest man in LA"), n_results=n_results)
result

# answer position: 1

result = db.query(embedder("Is vibranium tough"), n_results=n_results)
result

# answer position: 4

result = db.query(embedder("Is Casablanca a bad place to die"), n_results=n_results)
result

# answer position: none

result = db.query(embedder("Who is the heavyweight champion of the world"), n_results=10)
result

# answer position: any (4, 9)

result = db.query(embedder("What does hangar 51 contain"), n_results=5)
result

# answer position: 2

result = db.query(embedder("The youngest billionaire in the world"), n_results=5)
result

# answer position: 1

"""# Evaluation"""

class CollectorEvaluator:
  def __init__(self, collector: Collector, n_top=100):
    pass

  def explore_collector(self, text):
    pass

  def eval(self, query, answer):
    pass

  def calculate_statistics(self, data):
    pass

  def explore_and_calculate(self, data):
    pass

