{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OW1qo6q1xwZM"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoEk43jvyCP0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "path = '/content/drive/MyDrive'\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/bbc_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['labels'] = le.fit_transform(df['labels'])"
      ],
      "metadata": {
        "id": "RH-1dAaU4-WY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file4 = path + '/train.txt'\n",
        "file5 = path + '/test.txt'\n",
        "\n",
        "file4_l = path + '/train_labels.txt'\n",
        "file5_l = path + '/test_labels.txt'\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "    os.remove(file4)\n",
        "    os.remove(file5)\n",
        "    os.remove(file4_l)\n",
        "    os.remove(file5_l)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train, test = train_test_split(df, test_size=0.2)\n",
        "\n",
        "\n",
        "with open(file4, \"a\") as f:\n",
        "    for t in train['data']:\n",
        "        f.write(t + '\\n')\n",
        "        f.write('_____')\n",
        "\n",
        "with open(file5, \"a\") as f:\n",
        "    for t in test['data']:\n",
        "        f.write(t + '\\n')\n",
        "        f.write('_____')\n",
        "\n",
        "with open(file4_l, \"a\") as f:\n",
        "    for t in train['labels']:\n",
        "        f.write(str(t) + '\\n')\n",
        "\n",
        "with open(file5_l, \"a\") as f:\n",
        "    for t in test['labels']:\n",
        "        f.write(str(t) + '\\n')"
      ],
      "metadata": {
        "id": "dhnVxd7I3kAJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_sentences(text, file):\n",
        "\n",
        "    sents = []\n",
        "\n",
        "    text0 = re.sub('\\n', ' ', text)\n",
        "    text0 = re.sub('\\s$|\\\"', '', text0)\n",
        "\n",
        "    text1 = re.split('\\s{2,}', text0)\n",
        "    sents.append(text1[0]+'.')\n",
        "\n",
        "    text2 = ' '.join(text1[1:])\n",
        "\n",
        "    text3 = re.split('(?<=[.!?]) +', text2)\n",
        "\n",
        "    text3 = [t for t in text3 if t != '']\n",
        "\n",
        "    sents += text3\n",
        "\n",
        "    with open(file, \"a\") as f:\n",
        "        for line in sents:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "    return sents\n",
        "\n"
      ],
      "metadata": {
        "id": "z4Z2mOFuzIhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def split_into_words(sents, file):\n",
        "\n",
        "    words = []\n",
        "\n",
        "    for s in sents:\n",
        "\n",
        "        s0 = re.sub('xc2xa', '', s)\n",
        "\n",
        "        s0 = re.sub(' ', '__probel__', s0)\n",
        "\n",
        "        s0 = re.sub('(?<=(Dr|Mr|Ms))(__probel__)(?=[A-Z])', ' ', s0)\n",
        "\n",
        "\n",
        "        w = re.split('__probel__', s0)\n",
        "        w = [w_i for w_i in w if w_i != '' and w_i != '-']\n",
        "        words += w\n",
        "\n",
        "\n",
        "\n",
        "    with open(file, \"a\") as f:\n",
        "        for word in words:\n",
        "            f.write(word + '\\n')\n",
        "\n",
        "    return words\n"
      ],
      "metadata": {
        "id": "q8LUZj_gbFkY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file = path + '/sentences.txt'\n",
        "file2 = path + '/words.txt'\n",
        "file3 = path+'/records.tsv'\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "try:\n",
        "    os.remove(file)\n",
        "    os.remove(file2)\n",
        "    os.remove(file3)\n",
        "except OSError:\n",
        "    pass\n",
        "\n",
        "\n",
        "for i in range(len(df)):\n",
        "    sents = split_into_sentences(df['data'][i], file)\n",
        "    split_into_words(sents, file2)"
      ],
      "metadata": {
        "id": "xIv8f4clGTri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примеры, которые корректно обрабатываются:\n",
        "\n",
        "1.   speaker-climbing\n",
        "\n",
        "2.   $1300\n",
        "\n",
        "3.   9/11\n",
        "\n",
        "4.   26.5%\n",
        "\n",
        "5.   $15.5m\n",
        "\n",
        "6.   26th\n",
        "\n",
        "7.   my.post.444.@epost.ru\n",
        "\n",
        "8.   Ms | Mr | Dr\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "gZRs-P2SFdfB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "wnl = WordNetLemmatizer()\n",
        "\n",
        "file3 = path+'/records.tsv'\n",
        "\n",
        "\n",
        "def stem_and_lem(tokens):\n",
        "    snow_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "    stem_words = []\n",
        "    lem_words = []\n",
        "\n",
        "    for w in tokens:\n",
        "        x = snow_stemmer.stem(w)\n",
        "        y = wnl.lemmatize(w)\n",
        "        stem_words.append(x)\n",
        "        lem_words.append(y)\n",
        "\n",
        "\n",
        "    with open(file3, 'w', newline='') as tsvfile:\n",
        "        writer = csv.writer(tsvfile, delimiter='\\t', lineterminator='\\n')\n",
        "\n",
        "        need_empty_line = False\n",
        "        for t, s, l in zip(tokens, stem_words, lem_words):\n",
        "            if need_empty_line:\n",
        "                writer.writerow(\"\")\n",
        "                need_empty_line = False\n",
        "\n",
        "\n",
        "            if t[-1] in ('.', '!', '?', '...', ')', '\"', ','):\n",
        "                if t[-1] in ('.', '!', '?', '...'):\n",
        "                    need_empty_line = True\n",
        "                k = t[-1]\n",
        "                writer.writerow([t[:-1], s[:-1], l[:-1]])\n",
        "                writer.writerow([k, k, k])\n",
        "            elif t[0] in ('(', '\"'):\n",
        "                k = t[0]\n",
        "                writer.writerow([k, k, k])\n",
        "                writer.writerow([t[1:], s[1:], l[1:]])\n",
        "            else:\n",
        "                writer.writerow([t, s, l])\n",
        "\n",
        "\n",
        "        size = tsvfile.truncate()\n",
        "        size = tsvfile.truncate(size - 2)\n",
        "\n"
      ],
      "metadata": {
        "id": "6zwPfZ4Gw4qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(file2, \"r+\") as f:\n",
        "    lines = f.read().splitlines()\n",
        "    stem_and_lem(lines)"
      ],
      "metadata": {
        "id": "4uPmxRFeysGM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}