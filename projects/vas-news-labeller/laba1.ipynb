{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWdF7ZIsCEZq",
        "outputId": "82570679-c74e-4cf7-cbd6-279fb4c24524"
      },
      "outputs": [],
      "source": [
        "!wget http://qwone.com/~jason/20Newsgroups/20news-bydate.tar.gz\n",
        "!tar -xvzf ./20news-bydate.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PmDU84ihFLFQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def read_files_from_folders(root_folder):\n",
        "    all_files_content = []\n",
        "\n",
        "\n",
        "    for foldername, subfolders, filenames in os.walk(root_folder):\n",
        "        for filename in filenames:\n",
        "\n",
        "            file_path = os.path.join(foldername, filename)\n",
        "            try:\n",
        "\n",
        "                with open(file_path, 'r', errors='ignore') as file:\n",
        "                    content = file.read()\n",
        "                    content = content.replace('\\n', ' ')\n",
        "                    content = content.replace('\\t', ' ')\n",
        "                    all_files_content.append(content)\n",
        "            except Exception as e:\n",
        "                print(f\"Ошибка при чтении файла {file_path}: {e}\")\n",
        "\n",
        "    return all_files_content\n",
        "\n",
        "\n",
        "root_directory = '20news-bydate-test'\n",
        "all_contents = read_files_from_folders(root_directory)\n",
        "all_contents = ' '.join(all_contents)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "faTbF3oyHL7j"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJkuwbIJX-SZ",
        "outputId": "4695836f-72c1-412e-dd39-b531dc66df0e"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "sentence_pattern = r'([.!?])(?=\\s|$)'\n",
        "\n",
        "def split_sentences_with_end_symbols(text):\n",
        "    parts = re.split(sentence_pattern, text)\n",
        "\n",
        "    sentences = [''.join([parts[i], parts[i+1]]) for i in range(0, len(parts)-1, 2)]\n",
        "\n",
        "    return sentences\n",
        "\n",
        "text = all_contents\n",
        "\n",
        "sentences = split_sentences_with_end_symbols(text)\n",
        "for sentence in sentences:\n",
        "    print(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R3i1nz1D-FUD"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "word_pattern = r'\\b[\\w\\'-]+@[\\w.]+\\b|\\b[\\w\\'-]+\\b|\\d+|[^\\w\\s]'\n",
        "email_pattern = r'[a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+'\n",
        "phone_ru_pattern = r\"^(?:\\+7|8)(?:(?:-\\d{3}-|\\(\\d{3}\\))\\d{3}-\\d{2}-\\d{2}|\\d{10})\"\n",
        "phone_usa_pattern = r\"^(?:\\+1)(?:(?:-\\d{3}-|\\(\\d{3}\\))\\d{3}-\\d{4}|\\d{7})\"\n",
        "phone_china_pattern = r\"^(?:\\+86)(?:(?:-\\d{3}-|\\(\\d{3}\\))\\d{4}-\\d{4}|\\d{11})\"\n",
        "phone_pattern = r'(\\+\\d{1,3}\\s?)?(?:\\(\\d{3}\\)|\\d{1,3})\\s?-?\\s?\\d{3,4}\\s?-?\\s?\\d{3,4}'\n",
        "numeral_pattern = r\"[0-9]+((th)|(\\\\'s))\" #числительные типо 4th\n",
        "dates_pattern = r'\\d{1,3}[\\.|\\/]\\d{1,4}[\\.|\\/]\\d{1,4}'\n",
        "times_pattern =  r'\\d{1,2}\\:\\d{2}'\n",
        "\n",
        "word_re = re.compile(word_pattern)\n",
        "email_re = re.compile(email_pattern)\n",
        "phone_re = re.compile(f\"({phone_pattern})|({phone_ru_pattern})|({phone_usa_pattern})|({phone_china_pattern})\")\n",
        "numeral_re = re.compile(numeral_pattern)\n",
        "dates_re = re.compile(dates_pattern)\n",
        "times_re = re.compile(times_pattern)\n",
        "\n",
        "def tokenize_and_include_emails(sentence):\n",
        "    tokens = []\n",
        "    index = 0\n",
        "    while index < len(sentence):\n",
        "\n",
        "        email_match = email_re.match(sentence, index)\n",
        "        if email_match:\n",
        "            tokens.append(email_match.group())\n",
        "            index = email_match.end()\n",
        "            continue\n",
        "\n",
        "\n",
        "        phone_match = phone_re.match(sentence, index)\n",
        "        if phone_match:\n",
        "            tokens.append(phone_match.group())\n",
        "            index = phone_match.end()\n",
        "            continue\n",
        "\n",
        "        numeral_match = numeral_re.match(sentence, index)\n",
        "        if numeral_match:\n",
        "            tokens.append(numeral_match.group())\n",
        "            index = numeral_match.end()\n",
        "            continue\n",
        "\n",
        "        dates_match = dates_re.match(sentence, index)\n",
        "        if dates_match:\n",
        "            tokens.append(dates_match.group())\n",
        "            index = dates_match.end()\n",
        "            continue\n",
        "\n",
        "        time_match = times_re.match(sentence, index)\n",
        "        if time_match:\n",
        "            tokens.append(time_match.group())\n",
        "            index = time_match.end()\n",
        "            continue\n",
        "\n",
        "        word_match = word_re.match(sentence, index)\n",
        "        if word_match:\n",
        "          token = word_match.group()\n",
        "    # Удаление повторяющихся точек или подчеркиваний (но не одиночных)\n",
        "          token = re.sub(r'([._])\\1+', r'\\1', token)  # Замена последовательностей точек или подчеркиваний на одиночный символ\n",
        "          tokens.append(token)\n",
        "          index = word_match.end()\n",
        "          continue\n",
        "\n",
        "        index += 1\n",
        "\n",
        "    return tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPUWtmIaQ6Vp",
        "outputId": "1789dcd2-1df5-4c74-cc0e-22687510b873"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "all_sentences_tokens_and_emails = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    tokens_and_emails = tokenize_and_include_emails(sentence)\n",
        "    all_sentences_tokens_and_emails.append(tokens_and_emails)\n",
        "\n",
        "for i, tokens_and_emails in enumerate(all_sentences_tokens_and_emails):\n",
        "    print(f\"Sentence {i+1}:\")\n",
        "    print(tokens_and_emails)\n",
        "    print()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "alwlqwH4eT_k",
        "outputId": "2633fd98-2dab-48ee-cf72-222f4753f8db"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GT8e9GWleOWu",
        "outputId": "e6516e3c-7b3f-4d51-c5cc-29d2b3eff6eb"
      },
      "outputs": [],
      "source": [
        "def get_stub_stem(token):\n",
        "    stem = stemmer.stem(token)\n",
        "    return stem\n",
        "\n",
        "def get_stub_lemma(token):\n",
        "    lemma = lemmatizer.lemmatize(token)\n",
        "    return lemma\n",
        "\n",
        "def create_annotation(sentences):\n",
        "    annotations = []\n",
        "    for sentence_index, sentence in enumerate(sentences):\n",
        "        tokens_and_emails = tokenize_and_include_emails(sentence)\n",
        "        for token_index, token in enumerate(tokens_and_emails):\n",
        "            stem = get_stub_stem(token)\n",
        "            lemma = get_stub_lemma(token)\n",
        "            annotations.append(f\"{sentence_index + 1}_{token_index + 1}\\t{token}\\t{stem}\\t{lemma}\")\n",
        "        annotations.append(\"\")\n",
        "    return annotations\n",
        "\n",
        "annotations = create_annotation(sentences)\n",
        "\n",
        "with open('annotations.tsv', 'w') as file:\n",
        "    file.write(\"\\n\".join(annotations))\n",
        "\n",
        "print(\"Аннотация сохранена в 'annotations.tsv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "reBT5drq8orT"
      },
      "outputs": [],
      "source": [
        "if 'TestTokenizeFunction' in globals():\n",
        "    del globals()['TestTokenizeFunction']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DX5OaorA43Ta"
      },
      "outputs": [],
      "source": [
        "import unittest\n",
        "\n",
        "class TestTokenize(unittest.TestCase):\n",
        "    def test_basic_sentence(self):\n",
        "        sentence = \"Hello, my email is test@example.com and my phone is +79533668096.\"\n",
        "        expected_tokens = [\"Hello\", \"my\", \"email\", \"is\", \"test@example.com\", \"and\", \"my\", \"phone\", \"is\", \"+79533668096\", \".\"]\n",
        "        self.assertEqual(tokenize_and_include_emails(sentence), expected_tokens)\n",
        "\n",
        "    def test_dates_and_times(self):\n",
        "        sentence = \"The meeting is on 12/12/2023 at 10:30.\"\n",
        "        expected_tokens = [\"The\", \"meeting\", \"is\", \"on\", \"12/12/2023\", \"at\", \"10:30\", \".\"]\n",
        "        self.assertEqual(tokenize_and_include_emails(sentence), expected_tokens)\n",
        "\n",
        "    def test_numerals(self):\n",
        "        sentence = \"I have 2's apples and 3 oranges.\"\n",
        "        expected_tokens = [\"I\", \"have\", \"2's\", \"apples\", \"and\", \"3\", \"oranges\", \".\"]\n",
        "        self.assertEqual(tokenize_and_include_emails(sentence), expected_tokens)\n",
        "\n",
        "    def test_underscore_and_dot_removal(self):\n",
        "        sentence = \"This is  . an _ example.\"\n",
        "        expected_tokens = [\"This\", \"is\", \"an\", \"example\", \".\"]\n",
        "        self.assertEqual(tokenize_and_include_emails(sentence), expected_tokens)\n",
        "\n",
        "    def test_combined(self):\n",
        "        sentence = \"Email: user@example.com, phone: +86 138 0013 8000, date: 01/01/2024, time: 14:30.\"\n",
        "        expected_tokens = [\"Email\", \":\", \"user@example.com\", \"phone\", \"+86 138 0013 8000\", \"date\", \"01/01/2024\", \"time\", \"14:30\", \".\"]\n",
        "        self.assertEqual(tokenize_and_include_emails(sentence), expected_tokens)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
