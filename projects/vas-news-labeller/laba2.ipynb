{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qx1lpavV5syq"
      },
      "source": [
        "Коллокация - устойчивое сочетание\n",
        "\n",
        "Mepa t-score также учитывает частоту совместной встречаемости ключевого слова и его коллоката, отвечая на вопрос, насколько не случай-ной является сила ассоциации (связанности) между коллокатами:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2xcnKRFcTpF1",
        "outputId": "702139f2-4794-48fd-fc90-18bf2f22ac95"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import regex as re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from collections import defaultdict\n",
        "import math\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "\n",
        "sentences = list()\n",
        "sentence = list()\n",
        "with open('/content/annotations.tsv') as file:\n",
        "  for line in file:\n",
        "      if line != \"\\n\":\n",
        "        sentence.append(line.split('\\t')[1])\n",
        "      if line == \"\\n\" and sentence:\n",
        "        sentences.append(sentence)\n",
        "        sentence = list()\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o3u1mgbuXXSh"
      },
      "source": [
        "удаляем символы, приводим к нижнем урегистру , используем стоп слово\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5R0p0d0kWXoz"
      },
      "outputs": [],
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "clear_data = []\n",
        "\n",
        "import re\n",
        "\n",
        "def clean_sentence(sentence):\n",
        "    cleaned = []\n",
        "    for lemma in sentence:\n",
        "        cleaned_tek = re.sub(r\"(?<!\\w)\\.(?!\\w)|[^\\w\\s@._-]+\", \"\", lemma.lower())\n",
        "        if cleaned_tek and cleaned_tek not in stop_words:\n",
        "            cleaned.append(cleaned_tek)\n",
        "    return cleaned\n",
        "for sentence in sentences:\n",
        "    clear_data.append(clean_sentence(sentence))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a31Sv8S4Wivp"
      },
      "outputs": [],
      "source": [
        "trigrams = list()\n",
        "for sentence in clear_data:\n",
        "    for i in range(len(sentence) - 2):\n",
        "        trigrams.append(sentence[i:i+3])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXz3djpMYPRU"
      },
      "outputs": [],
      "source": [
        "#считаем частоты\n",
        "\n",
        "trigram_frequencie = dict()\n",
        "for trigram in trigrams:\n",
        "    if tuple(trigram) not in trigram_frequencie:\n",
        "        trigram_frequencie[tuple(trigram)] = 1\n",
        "    else:\n",
        "        trigram_frequencie[tuple(trigram)] += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HbEGbSDWYwQk"
      },
      "outputs": [],
      "source": [
        "\n",
        "def t_score(trigram_frequencies, word_counts, uniq_words):\n",
        "    tscores = {}\n",
        "    for trigram, count in trigram_frequencies.items():\n",
        "        expected_frequency = (word_counts[trigram[0]] / uniq_words) * (word_counts[trigram[1]] / uniq_words) *(word_counts[trigram[2]] / uniq_words) / uniq_words\n",
        "        tscore = (count - expected_frequency) / math.sqrt(count)\n",
        "\n",
        "        tscores[trigram] = tscore\n",
        "    return tscores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hj417jaPYxHK"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "words_count = Counter()\n",
        "\n",
        "for sentence in clear_data:\n",
        "    words_count.update(sentence)  #сколько раз встречается слово\n",
        "\n",
        "uniq_words_total = sum(words_count.values()) #колво уникальных слов\n",
        "\n",
        "t_scores = t_score(trigram_frequencie, words_count, uniq_words_total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cF4BYdJmZA5H",
        "outputId": "601146c4-56f4-466b-bcbf-554bab584801"
      },
      "outputs": [],
      "source": [
        "sorted(t_scores.items(), key=lambda x: x[1], reverse=True)[:30]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfwXk7jFZxED",
        "outputId": "23bd3177-38db-45ec-db01-90caae255ba1"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.collocations import *\n",
        "from nltk.corpus import PlaintextCorpusReader\n",
        "\n",
        "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "\n",
        "\n",
        "\n",
        "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
        "\n",
        "flat_list = [item for sublist in clear_data for item in sublist]\n",
        "finder_thr = TrigramCollocationFinder.from_words(flat_list)\n",
        "print(finder_thr.nbest(trigram_measures.student_t, 30))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VwmefDw0Z49O",
        "outputId": "e62e75ea-4cc7-4a9a-a2cc-49c433c1580e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "def calculate_matrices(trigram_frequencies, word_counts, total_words):\n",
        "    # Получаем список уникальных слов\n",
        "    unique_words = list(word_counts.keys())\n",
        "    n = len(unique_words)\n",
        "\n",
        "    # Инициализация матрицы частот\n",
        "    observed_matrix = np.zeros((n, n, n))\n",
        "    expected_matrix = np.zeros((n, n, n))\n",
        "\n",
        "    # Заполнение наблюдаемой матрицы\n",
        "    for trigram, count in trigram_frequencies.items():\n",
        "        i = unique_words.index(trigram[0])\n",
        "        j = unique_words.index(trigram[1])\n",
        "        k = unique_words.index(trigram[2])\n",
        "        observed_matrix[i, j, k] = count\n",
        "\n",
        "    # Заполнение ожидаемой матрицы\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            for k in range(n):\n",
        "                expected_frequency = (\n",
        "                    (word_counts[unique_words[i]] / total_words) *\n",
        "                    (word_counts[unique_words[j]] / total_words) *\n",
        "                    (word_counts[unique_words[k]] / total_words) *\n",
        "                    total_words\n",
        "                )\n",
        "                print(expected_frequency)\n",
        "                expected_matrix[i, j, k] = expected_frequency\n",
        "\n",
        "    return observed_matrix, expected_matrix\n",
        "\n",
        "def log_likelihood(observed_matrix, expected_matrix):\n",
        "    ll_scores = 0\n",
        "    for i in range(observed_matrix.shape[0]):\n",
        "        for j in range(observed_matrix.shape[1]):\n",
        "            for k in range(observed_matrix.shape[2]):\n",
        "                O = observed_matrix[i, j, k]\n",
        "                E = expected_matrix[i, j, k]\n",
        "                if O > 0 and E > 0:\n",
        "                    ll_scores += O * math.log(O / E)\n",
        "    return 2 * ll_scores\n",
        "\n",
        "# Пример использования\n",
        "observed_matrix, expected_matrix = calculate_matrices(trigram_frequencie, words_count, uniq_words_total)\n",
        "ll_score = log_likelihood(observed_matrix, expected_matrix)\n",
        "print(ll_score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
