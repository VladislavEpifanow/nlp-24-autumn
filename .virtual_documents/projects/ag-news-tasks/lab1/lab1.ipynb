import numpy as np
import pandas as pd
import nltk
import os


nltk.download('wordnet')
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer


stemmer = SnowballStemmer(language='english')
lemmatizer = WordNetLemmatizer()


import re


train_data = pd.read_parquet("../data/train.parquet")


test_data = pd.read_parquet("../data/test.parquet")


def split_into_sentences(text):
    # (?<!\w\.\w.) - проверяет, что нет слова, за которым следует точка и еще одно слово.
    # (?<![A-Z][a-z]\.) - проверяет, что перед текущей позицией нет заглавной буквы, за которой следует строчная буква и точка.
    # (?<=\.|\?|!) - проверяет, что перед текущей позицией находится точка, вопросительный знак или восклицательный знак.
    # \s - пробельный символ 
    sentence_pattern = re.compile(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|!)\s')
    
    # Находим все предложения в тексте
    sentences = sentence_pattern.split(text)
    
    # Удаляем пустые строки, если они есть
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
    
    return sentences


email_pattern = r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b'


def find_emails(sentence):
    email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
    
    # Находим все email-адреса в тексте
    emails = email_pattern.findall(sentence)
    
    return emails


text = 'Please, send your feedback at this adress: {mail}'
emails = ["user.name@example.com","another-email@domain.org", "test123@sub.domain.co.uk"]
for em in emails:
    print(find_emails(text.format(mail=em)))


def find_phone_number(sentence):
    number_pattern = re.compile(r'\+?7?[- (]?\d{3}[- )]?\d{3}[- ]?\d{2}[- ]?\d{2}')
    
    # Находим все номера телефонов в тексте
    phone_numbers = number_pattern.findall(sentence)
    
    return phone_numbers


text_phone = 'Please, call us back: {number}'
phones = ["+79261234567","89261234567", "79261234567","+7 926 123 45 67", "8(926)123-45-67"]
for phone in phones:
    print(find_phone_number(text_phone.format(number=phone)))


text_few_phones = """Please, call us, to get more information: 
Main phone number: +7 926 123 45 67
Additional phone number: +79261234567"""
find_phone_number(text_few_phones)


def find_dates(sentence):
    # Регулярное выражение для поиска дат
    date_pattern = re.compile(r'\b(\d{1,2})([./-]?)(\d{1,2})\2(\d{2,4})\b')
    
    # Находим все даты в тексте
    dates = date_pattern.findall(sentence)

    # Преобразуем найденные даты в строки
    formatted_dates = [f"{day}{separator}{month}{separator}{year}" for day, separator, month, year in dates]
    
    return formatted_dates


text_date = 'Date to meet: {dat}'
dates = ['19.06.2024', '19-06-2024', '10/12/24']
for date in dates:
    print(find_dates(text_date.format(dat=date)))


def extract_english_words(sentence):
    # Регулярное выражение для выделения английских слов
    word_pattern = re.compile(r'\b[A-Za-z\']+\b')
    
    # Находим все английские слова в тексте
    words = word_pattern.findall(sentence)
    
    return words


text = """
This is an example text with some English words like "don't", "it's", and "Python".
Also, there are some numbers like 123 and special characters like @#$%.
"""

english_words = extract_english_words(text)
tokens = english_words


stemmer.stem('numbers')


lemmatizer.lemmatize('numbers')


def tokenizer(text):
    sentences = split_into_sentences(text)
    annotated_text = ''
    for sentence in sentences:
        annotated_sentences = ''
        word_tokens = extract_english_words(sentence)

        for token in word_tokens:
            stem = stemmer.stem(token)
            lemm = lemmatizer.lemmatize(token)

            annotated_sentences += '\t'.join([token, stem, lemm]) + '\n'


        emails = find_emails(sentence)
        phones = find_phone_number(sentence)
        dates = find_dates(sentence)
        if emails:
            for email in emails:
                annotated_sentences += email + '\n'
        if phones:
            for phone in phones:
                annotated_sentences += phone + '\n'
        if dates:
            for date in dates:
                annotated_sentences += date + '\n'

        annotated_text += annotated_sentences + '\n'

    return annotated_text


%%time
tst += tokenizer(test_data['text'][56])


sorted(train_data['label'].unique())[0]


train_data.loc[train_data['label'] == 3]['text'].to_list()[0]





train_data.shape


os.path.exists('../assets/annotated-corpus/train/1/')


os.makedirs('../assets/annotated-corpus/train/1/')


with open('../assets/annotated-corpus/train/1/1.tsv', 'w', encoding='utf-8') as tsv_file:
    tsv_file.write(tst)


def generate_corpus(train_data, test_data):
    path_to_save = '../assets/annotated-corpus/'
    classes = sorted(train_data['label'].unique())
    for type, data in [('train', train_data), ('test',test_data)]:
        for cl in classes:
            folder_name = path_to_save + type + '/' + str(cl)
            annotated_file = ''
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
                
            for text_item in data.loc[data['label'] == cl]['text'].to_list():
                annotated_file += tokenizer(text_item)
                
            file_path = folder_name + '/' + str(cl) + '.tsv'
            with open(file_path, 'w', encoding='utf-8') as tsv_file:
                tsv_file.write(annotated_file)
                


generate_corpus(train_data, test_data)



