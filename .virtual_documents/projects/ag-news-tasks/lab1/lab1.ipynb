import numpy as np
import pandas as pd
import nltk
import os


# nltk.download('wordnet')
from nltk.stem.snowball import SnowballStemmer
from nltk.stem import WordNetLemmatizer


stemmer = SnowballStemmer(language='english')
lemmatizer = WordNetLemmatizer()


import re


train_data = pd.read_parquet("../data/train.parquet")


test_data = pd.read_parquet("../data/test.parquet")


def split_into_sentences(text):
    # (?<!\w\.\w.) - проверяет, что нет слова, за которым следует точка и еще одно слово.
    # (?<![A-Z][a-z]\.) - проверяет, что перед текущей позицией нет заглавной буквы, за которой следует строчная буква и точка.
    # (?<=\.|\?|!) - проверяет, что перед текущей позицией находится точка, вопросительный знак или восклицательный знак.
    # \s - пробельный символ 
    sentence_pattern = re.compile(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|!)\s')
    
    # Находим все предложения в тексте
    sentences = sentence_pattern.split(text)
    
    # Удаляем пустые строки, если они есть
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]
    
    return sentences


split_into_sentences("Who are you? How it's going?")


def find_emails(sentence):
    # [A-Za-z0-9._%+-]+ - часть почты до @
    # [A-Za-z0-9.-]+ - доменная часть - .com(или другого)
    # [A-Z|a-z]{2,} - доменный уровень (например, .com, .org)
    email_pattern = re.compile(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b')
    
    # Находим все email-адреса в тексте
    emails = email_pattern.findall(sentence)
    
    return emails


text = 'Please, send your feedback at this adress: {mail}'
emails = ["user.name@example.com","another-email@domain.org", "test123@sub.domain.co.uk"]
for em in emails:
    print(find_emails(text.format(mail=em)))


def find_phone_number(sentence):
    # \+? - символ + есть ноль или 1 раз
    # [- (]? и [- )]? - разделитель в  виде -, () и пробел
    # \d{3} - три любых цифры
    # 
    number_pattern = re.compile(r'\+?7?[- (]?\d{3}[- )]?\d{3}[- ]?\d{2}[- ]?\d{2}')
    
    # Находим все номера телефонов в тексте
    phone_numbers = number_pattern.findall(sentence)
    
    return phone_numbers


text_phone = 'Please, call us back: {number}'
phones = ["+79261234567","89261234567", "79261234567","+7 926 123 45 67", "8(926)123-45-67"]
for phone in phones:
    print(find_phone_number(text_phone.format(number=phone)))


text_phones = """Please, call us, to get more information:
Additional phone number: +79261234567
Email: mail@yandex.com"""
find_phone_number(text_phones)


def find_dates(sentence):
    # Регулярное выражение для поиска дат
    date_pattern = re.compile(r'\b(\d{1,2})([./-]?)(\d{1,2})\2(\d{2,4})\b')
    
    # Находим все даты в тексте
    dates = date_pattern.findall(sentence)

    # Преобразуем найденные даты в строки
    formatted_dates = [f"{day}{separator}{month}{separator}{year}" for day, separator, month, year in dates]
    
    return formatted_dates


text_date = 'Date to meet: {dat}'
dates = ['19.06.2024', '19-06-2024', '10/12/24']
for date in dates:
    print(find_dates(text_date.format(dat=date)))


def tokenize_sentence(sentence):
    # Регулярное выражение для выделения отдельных токенов
    pattern = r"\+?\b[\w@.]+(?:'\w+)?\b|[:;,?.!]"
    return re.findall(pattern, sentence)


tokens = tokenize_sentence(text_phones)


print(text_phones)


print(tokens)


stemmer.stem('refridgerator')


stemmer.stem('generations')


stemmer.stem('optimization')


lemmatizer.lemmatize('refridgerator')


lemmatizer.lemmatize('generation')


def tokenizer(text):
    sentences = split_into_sentences(text)
    annotated_text = ''
    for sentence in sentences:
        annotated_sentences = ''
        word_tokens = tokenize_sentence(sentence)

        for token in word_tokens:
            emails = find_emails(token)
            phones = find_phone_number(token)
            dates = find_dates(token)
            if emails:
                annotated_sentences += email[0] + '\n'
            elif phones:
                annotated_sentences += phone[0] + '\n'
            elif dates:
                annotated_sentences += dates[0] + '\n'
            else:
                stem = stemmer.stem(token)
                lemm = lemmatizer.lemmatize(token)
                annotated_sentences += '\t'.join([token, stem, lemm]) + '\n'
        
        annotated_text += annotated_sentences + '\n'

    return annotated_text


def tokenize_text(text):
    sentences = split_into_sentences(text)
    tokenized = []
    
    for sentence in sentences:
        sentence_tokens = []
        word_tokens = tokenize_sentence(sentence)
        
        for token in word_tokens:
            # Поиск специальных сущностей
            if emails := find_emails(token):
                sentence_tokens.append([emails[0]])
            elif phones := find_phone_number(token):
                sentence_tokens.append([phones[0]])
            elif dates := find_dates(token):
                sentence_tokens.append([dates[0]])
            else:
                # Обычная обработка токена
                stem = stemmer.stem(token)
                lemma = lemmatizer.lemmatize(token)
                sentence_tokens.append([token, stem, lemma])
        
        tokenized.append(sentence_tokens)
    
    return tokenized

def format_to_tsv(tokenized_data):
    tsv_lines = []
    
    for sentence in tokenized_data:
        # Обрабатываем токены в предложении
        for token_info in sentence:
            if len(token_info) == 1:  # Специальные сущности
                tsv_lines.append(token_info[0])
            else:  # Обычные токены
                tsv_lines.append('\t'.join(token_info))
        
        # Добавляем разделитель предложений
        tsv_lines.append('')
    
    # Убираем последний лишний разделитель
    return '\n'.join(tsv_lines[:-1])


tokenize_text(train_data['text'][5])[0]


print(train_data['text'][5])


print(tokenizer(train_data['text'][5]))


def generate_corpus(train_data, test_data):
    path_to_save = '../assets/annotated-corpus/'
    classes = sorted(train_data['label'].unique())
    for type, data in [('train', train_data), ('test',test_data)]:
        for cl in classes:
            folder_name = path_to_save + type + '/' + str(cl)
            annotated_file = ''
            if not os.path.exists(folder_name):
                os.makedirs(folder_name)
                
            for text_item in data.loc[data['label'] == cl]['text'].to_list():
                annotated_file += tokenizer(text_item)
                
            file_path = folder_name + '/' + str(cl) + '.tsv'
            with open(file_path, 'w', encoding='utf-8') as tsv_file:
                tsv_file.write(annotated_file)
                


import os

def generate_corpus(train_data, test_data):
    path_to_save = '../assets/annotated-corpus/'
    # Получаем все уникальные классы из train и test
    train_classes = set(train_data['label'].unique())
    test_classes = set(test_data['label'].unique())
    all_classes = sorted(list(train_classes.union(test_classes)))
    
    for data_type, data in [('train', train_data), ('test', test_data)]:
        # Создаем копию данных, чтобы не изменять оригинальные
        data = data.copy()
        # Проверяем наличие doc_id, генерируем при необходимости
        if 'doc_id' not in data.columns:
            data.reset_index(drop=True, inplace=True)
            data['doc_id'] = data.index.astype(str)
        
        # Проходим по всем классам
        for cl in all_classes:
            # Фильтруем данные по текущему классу
            class_data = data[data['label'] == cl]
            if class_data.empty:
                continue  # Пропускаем, если нет документов
            
            # Создаем папку для класса
            class_folder = os.path.join(path_to_save, data_type, str(cl))
            os.makedirs(class_folder, exist_ok=True)
            
            # Сохраняем каждый документ отдельным файлом
            for _, row in class_data.iterrows():
                doc_id = row['doc_id']
                text_content = tokenizer(row['text'])
                file_path = os.path.join(class_folder, f"{doc_id}.tsv")
                with open(file_path, 'w', encoding='utf-8') as file:
                    file.write(text_content)


generate_corpus(train_data, test_data)



