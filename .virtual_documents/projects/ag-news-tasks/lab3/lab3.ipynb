from gensim.models import Word2Vec
import pandas as pd
import re
import nltk
from nltk.corpus import stopwords
import os
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm


nltk.download('stopwords')


train_data = []
test_data = []


def prepare_data(tokens):
    pattern = r'[^\w\s]'
    stop_words = set(stopwords.words('english'))
    
    cleaned_tokens = []
    for token in tokens:
        clean_token = re.sub(pattern, '', str(token).lower())
        if clean_token != '' and clean_token not in stop_words:
            cleaned_tokens.append(clean_token)
        
    return cleaned_tokens


def read_tsv_sentences(file_path: str) -> list[list[str]]:
    df = pd.read_csv(file_path, sep='\t', usecols=[0], header=None, names=['word'], skip_blank_lines=False, na_filter=False)
    words = df['word'].astype(str).str.strip().tolist()
    
    sentences = []
    current_sentence = []
    
    for word in words:
        if not word:  # Пустая строка в первом столбце
            if current_sentence:
                sentences.append(current_sentence)
                current_sentence = []
        else:
            current_sentence.append(word)
    
    # Добавляем последнее предложение, если оно есть
    if current_sentence:
        sentences.append(current_sentence)
    
    return sentences

# Пример использования
sentences = read_tsv_sentences(folder_path)


# Инициализация структур для данных
train_data = []
test_data = []
base_path = '../assets/annotated-corpus/'

# Обработка данных
for data_type in tqdm(['train', 'test'], desc='Datasets'):
    data_path = os.path.join(base_path, data_type)
    
    if not os.path.exists(data_path):
        continue
        
    # Получаем список классов из поддиректорий
    classes = [
        d for d in os.listdir(data_path) 
        if os.path.isdir(os.path.join(data_path, d))
    ]
    
    for class_name in tqdm(classes, desc=f'Classes ({data_type})'):
        class_path = os.path.join(data_path, class_name)
        
        # Получаем все TSV-файлы в директории класса
        docs = [
            f for f in os.listdir(class_path) 
            if f.endswith('.tsv') and os.path.isfile(os.path.join(class_path, f))
        ]
        
        for doc_name in tqdm(docs, desc='Documents'):
            doc_path = os.path.join(class_path, doc_name)
            sentences = read_tsv_sentences(doc_path)
            
            if data_type == 'train':
                train_data.extend(sentences)
            else:
                test_data.extend(sentences)


for data_type in tqdm(['train', 'test']):
    for folder in tqdm(['0', '1', '2', '3']):
        folder_path = f'../assets/annotated-corpus/{data_type}/{folder}/{folder}.tsv'
        data = read_tsv_sentences(folder_path)
        
        if data_type == 'train':
            train_data.extend(data)
        else:
            test_data.extend(data)
            


model_path = "word2vec.model"


def train_word2vec(train_data, model_path='word2vec.model'):    
    model = Word2Vec(
        vector_size=100,
        window=5,
        min_count=1,
        workers=4,
        sg=1,
        alpha=0.025,
        min_alpha=0.0001
    )
    
    print("\nПостроение словаря...")
    model.build_vocab(train_data)
    
    print("\nНачало обучения...")
    model.train(
        train_data,
        total_examples=model.corpus_count,
        epochs=10,
        compute_loss=True,
        report_delay=1
    )
    
    model.save(model_path)
    print(f"\nМодель сохранена в {model_path}")
    print(f"Размер словаря: {len(model.wv.key_to_index)}")
    print(f"Примеры слов: {list(model.wv.key_to_index.keys())[:10]}")
    
    return model


model = train_word2vec(train_data)


# Функция для векторизации текста
def vectorize_text(text, model):
    # Предобработка входного текста
    processed_text = prepare_data(text.split())
    
    # Получаем векторы для всех слов из текста, присутствующих в модели
    word_vectors = [model.wv[word] for word in processed_text if word in model.wv]
    
    if len(word_vectors) == 0:
        return np.zeros(model.vector_size)
    
    # Усредняем векторы слов для получения вектора текста
    text_vector = np.mean(word_vectors, axis=0)
    return text_vector, word_vectors


from typing import Union, List

def get_word_vectors(words, model, zero_vector_for_unknown=True):
    if isinstance(words, str):
        words = [words]
        
    vectors = []
    vector_size = model.vector_size if hasattr(model, 'vector_size') else 100
    
    for word in words:
        if word in model.wv:
            vectors.append(model.wv[word])
        else:
            if zero_vector_for_unknown:
                vectors.append(np.zeros(vector_size))
            else:
                continue  # Пропускаем отсутствующие слова
    
    return vectors


# Пример использования
sample_text = "There is an example of sample text to be vectorize"
vectorized_text, word_vectors = vectorize_text(sample_text, model)
print(f"Vectorized text shape: {vectorized_text.shape}")


words = ['cat', 'dog', 'animal']
get_word_vectors(words, model)


# Реализация косинусного расстояния
def cosine_distance(vec1: np.ndarray, vec2: np.ndarray) -> float:
    """Собственная реализация косинусного расстояния"""
    dot_product = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    
    if norm1 == 0 or norm2 == 0:
        return 1.0  # Максимальное расстояние для нулевых векторов
    
    return 1 - (dot_product / (norm1 * norm2))


test_data[138]


# Тестовые данные для анализа
test_words = {
    'press': {
        'similar': ['media', 'journalism', 'news'],
        'related': ['article', 'reporter', 'headline'],
        'different': ['mountain', 'guitar', 'painting']
    },
    'government': {
        'similar': ['administration', 'authority', 'regime'],
        'related': ['politics', 'law', 'policy'],
        'different': ['flower', 'bicycle', 'music']
    },
    'crime': {
        'similar': ['offense', 'violation', 'felony'],
        'related': ['police', 'law', 'punishment'],
        'different': ['sunshine', 'happiness', 'rainbow']
    },
    'troops': {
        'similar': ['soldiers', 'military', 'army'],
        'related': ['war', 'defense', 'combat'],
        'different': ['peace', 'butterfly', 'garden']
    }
}


# Функция для анализа расстояний
def analyze_distances(word, model, test_words):
    if word not in model.wv:
        raise ValueError(f"Слово '{word}' отсутствует в модели")
    
    main_vector = model.wv[word]
    distances = {'similar': [], 'related': [], 'different': []}
    
    for group, words in test_words[word].items():
        for w in words:
            if w in model.wv:
                dist = cosine_distance(main_vector, model.wv[w])
                distances[group].append(dist)
    
    return distances


metrics = {}
for word in test_words:
    distances = analyze_distances(word, model, test_words)
    metrics[word] = {
        'similar_mean': np.mean(distances['similar']),
        'related_mean': np.mean(distances['related']),
        'different_mean': np.mean(distances['different']),
        'diff_score': np.mean(distances['different']) - np.mean(distances['similar'])
    }


metrics



